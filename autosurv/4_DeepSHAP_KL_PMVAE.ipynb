{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65023bce-6b39-4117-b7f3-39df42d9b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from KL_PMVAE_shap import KL_PMVAE_2omics\n",
    "from utils import get_match_id\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a112ae1e-bb22-4491-95a6-a02b558d556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def load_data_deepshap(data1, data2, dtype):\n",
    "    EXPR1 = data1.drop([\"patient_id\", \"OS\", \"OS.time\", \"age\", \"race_white\", \"stage_i\", \"stage_ii\"], axis = 1).values.astype(np.float64)\n",
    "    EXPR2 = data2.drop([\"patient_id\", \"OS\", \"OS.time\", \"age\", \"race_white\", \"stage_i\", \"stage_ii\"], axis = 1).values.astype(np.float64)\n",
    "    EXPR1 = torch.from_numpy(EXPR1).type(dtype)\n",
    "    EXPR2 = torch.from_numpy(EXPR2).type(dtype)\n",
    "        \n",
    "    return (EXPR1, EXPR2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def load_pathway(path, dtype):\n",
    "    '''Load a bi-adjacency matrix of pathways, and then covert it to a Pytorch tensor.\n",
    "    Input:\n",
    "        path: path to input dataset (which is expected to be a csv file).\n",
    "        dtype: define the data type of tensor (i.e. dtype=torch.FloatTensor)\n",
    "    Output:\n",
    "        PATHWAY_MASK: a Pytorch tensor of the bi-adjacency matrix of pathways & genes.\n",
    "    '''\n",
    "    pathway_mask = pd.read_csv(path, index_col = 0).to_numpy()\n",
    "\n",
    "    PATHWAY_MASK = torch.from_numpy(pathway_mask).type(dtype)\n",
    "    PATHWAY_MASK = torch.transpose(PATHWAY_MASK, 0, 1)\n",
    "    \n",
    "    return(PATHWAY_MASK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa234c3c-12f6-4289-99cc-4e3e40a74dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "train_data_gene = pd.read_csv(\"processed_data_example/TCGA_BRCA/train_test_split/minmax_normalized/data_train_gene_minmax_overall.csv\")\n",
    "train_data_mirna = pd.read_csv(\"processed_data_example/TCGA_BRCA/train_test_split/minmax_normalized/data_train_mirna_minmax_overall.csv\")\n",
    "\n",
    "gene_names = list(train_data_gene.columns)[7:]\n",
    "mirna_names = list(train_data_mirna.columns)[7:]\n",
    "feature_names = gene_names + mirna_names\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "upper_data = pd.read_csv(\"saved_models/higher_PI_train.csv\")\n",
    "lower_data = pd.read_csv(\"saved_models/lower_PI_train.csv\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "pathway_mask = pd.read_csv(\"processed_data_example/TCGA_BRCA/train_test_split/minmax_normalized/pathway_mask.csv\", index_col = 0)\n",
    "pathway_names = list(pathway_mask.columns)\n",
    "mask_names = pathway_names + mirna_names\n",
    "pathway_mask_int = load_pathway(\"processed_data_example/TCGA_BRCA/train_test_split/minmax_normalized/pathway_mask.csv\", dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c4f8a2a-9de3-4fe0-b4b1-d4db780ee1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Breast cancer dataset used as example here\n",
    "\"\"\"\n",
    "# number of genes\n",
    "input_n1 = 2699\n",
    "# number of miRNAs\n",
    "input_n2 = 516\n",
    "# number of latent features: one of the hyperparameters, determined via grid search during training/validation process\n",
    "z_dim = 16\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85eb9be1-b4b9-4a36-89f2-0a157481d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DeepSHAP\n",
    "\n",
    "These codes were adapted from the work of Withnell et al. https://academic.oup.com/bib/article/22/6/bbab315/6353242\n",
    "Please check their original implementation of DeepSHAP for more details at https://github.com/zhangxiaoyu11/XOmiVAE\n",
    "\"\"\"\n",
    "\n",
    "class Explainer(object):\n",
    "    \"\"\" This is the superclass of all explainers.\n",
    "    \"\"\"\n",
    "\n",
    "    def shap_values(self, X):\n",
    "        raise Exception(\"SHAP values not implemented for this explainer!\")\n",
    "\n",
    "    def attributions(self, X):\n",
    "        return self.shap_values(X)\n",
    "\n",
    "class PyTorchDeepExplainer(Explainer):\n",
    "    \"\"\"\n",
    "    This class has been adapted to explain OmiVAE. It is important that the correct output of the model\n",
    "    (0=z dimension, 2=mean) as well as the dimension within this latent space. We allow a dimension to be chosen to explain from, and adjustable outputs to be explained. \n",
    "    Lundberg et al., 2017: http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data1, data2, outputNumber, dim, explainLatentSpace):\n",
    "        \n",
    "        #data = list(load_data_deepshap(data, dtype))\n",
    "        data = list(load_data_deepshap(data1, data2, dtype))\n",
    "        \n",
    "        # check if we have multiple inputs\n",
    "        self.multi_input = False\n",
    "        if type(data) == list:\n",
    "            self.multi_input = True\n",
    "        else:\n",
    "            data = [data]\n",
    "        self.data = data\n",
    "        self.layer = None\n",
    "        self.input_handle = None\n",
    "        self.interim = False\n",
    "        self.interim_inputs_shape = None\n",
    "        self.expected_value = None  # to keep the DeepExplainer base happy\n",
    "        if type(model) == tuple:\n",
    "\n",
    "            self.interim = True\n",
    "            model, layer = model\n",
    "            model = model.eval()\n",
    "            self.layer = layer\n",
    "            self.add_target_handle(self.layer)\n",
    "\n",
    "            # if we are taking an interim layer, the 'data' is going to be the input\n",
    "            # of the interim layer; we will capture this using a forward hook\n",
    "            with torch.no_grad():\n",
    "                _ = model(*data)\n",
    "                #model.e_fc1.weight.data = model.e_fc1.weight.data.mul(model.pathway_mask)\n",
    "                #_ = model.relu(model.e_bn1(model.e_fc1(data[0])))\n",
    "                interim_inputs = self.layer.target_input\n",
    "                #print(\"Interim input type: %s.\" %type(interim_inputs))\n",
    "                if type(interim_inputs) is tuple:\n",
    "                    # this should always be true, but just to be safe\n",
    "                    self.interim_inputs_shape = [i.shape for i in interim_inputs]\n",
    "                else:\n",
    "                    self.interim_inputs_shape = [interim_inputs.shape]\n",
    "            self.target_handle.remove()\n",
    "            del self.layer.target_input\n",
    "        self.model = model.eval()\n",
    "        self.multi_output = False\n",
    "        self.num_outputs = 1\n",
    "        with torch.no_grad():\n",
    "            outputs = model(*data)\n",
    "            #model.e_fc1.weight.data = model.e_fc1.weight.data.mul(model.pathway_mask)\n",
    "            #outputs = model.relu(model.e_bn1(model.e_fc1(data[0])))\n",
    "            #This is where specifies whether we want to explain the mean or z output\n",
    "            if type(outputs) != tuple:\n",
    "                output = outputs\n",
    "            else:\n",
    "                output = outputs[outputNumber]\n",
    "            self.outputNum=outputNumber\n",
    "            # Chosen dimension\n",
    "            self.dim=None\n",
    "            self.explainLatent = False\n",
    "            if explainLatentSpace:\n",
    "                self.explainLatent=True\n",
    "                self.dimension=dim\n",
    "                output = output[:, dim]\n",
    "                output = output.reshape(output.shape[0], 1)\n",
    "            # also get the device everything is running on\n",
    "            self.device = output.device\n",
    "            if output.shape[1] > 1:\n",
    "                self.multi_output = True\n",
    "                self.num_outputs = output.shape[1]\n",
    "            self.expected_value = output.mean(0).cpu().numpy()\n",
    "\n",
    "    def add_target_handle(self, layer):\n",
    "\n",
    "        input_handle = layer.register_forward_hook(get_target_input)\n",
    "        self.target_handle = input_handle\n",
    "\n",
    "    def add_handles(self, model, forward_handle, backward_handle):\n",
    "        \"\"\"\n",
    "        Add handles to all non-container layers in the model.\n",
    "        Recursively for non-container layers\n",
    "        \"\"\"\n",
    "        handles_list = []\n",
    "        model_children = list(model.children())\n",
    "        if model_children:\n",
    "            for child in model_children:\n",
    "                handles_list.extend(self.add_handles(child, forward_handle, backward_handle))\n",
    "        else:  # leaves\n",
    "            handles_list.append(model.register_forward_hook(forward_handle))\n",
    "            handles_list.append(model.register_backward_hook(backward_handle))\n",
    "\n",
    "        return handles_list\n",
    "\n",
    "    def remove_attributes(self, model):\n",
    "        \"\"\"\n",
    "        Removes the x and y attributes which were added by the forward handles\n",
    "        Recursively searches for non-container layers\n",
    "        \"\"\"\n",
    "        for child in model.children():\n",
    "            if 'nn.modules.container' in str(type(child)):\n",
    "                self.remove_attributes(child)\n",
    "            else:\n",
    "                try:\n",
    "                    del child.x\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                try:\n",
    "                    del child.y\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "    def gradient(self, idx, inputs):\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        X = [x.requires_grad_() for x in inputs]\n",
    "        \n",
    "        output = self.model(*X)\n",
    "        #self.model.e_fc1.weight.data = self.model.e_fc1.weight.data.mul(self.model.pathway_mask)\n",
    "        #output = self.model.relu(self.model.e_bn1(self.model.e_fc1(X[0])))\n",
    "\n",
    "        #Specify the output to change\n",
    "        if type(output) != tuple:\n",
    "            outputs = output\n",
    "        else:\n",
    "            outputs = output[self.outputNum]\n",
    "\n",
    "        #Specify the dimension to explain\n",
    "        if self.explainLatent==True:\n",
    "\n",
    "            outputs = outputs[:, self.dimension]\n",
    "            outputs = outputs.reshape(outputs.shape[0], 1)\n",
    "\n",
    "\n",
    "        selected = [val for val in outputs[:, idx]]\n",
    "\n",
    "        grads = []\n",
    "        if self.interim:\n",
    "            interim_inputs = self.layer.target_input\n",
    "            for idx, input in enumerate(interim_inputs):\n",
    "                grad = torch.autograd.grad(selected, input,\n",
    "                                           retain_graph=True if idx + 1 < len(interim_inputs) else None,\n",
    "                                           allow_unused=True)[0]\n",
    "                if grad is not None:\n",
    "                    grad = grad.cpu().numpy()\n",
    "                else:\n",
    "                    grad = torch.zeros_like(interim_inputs[idx]).cpu().numpy()\n",
    "                grads.append(grad)\n",
    "            del self.layer.target_input\n",
    "            return grads, [i.detach().cpu().numpy() for i in interim_inputs]\n",
    "        else:\n",
    "            for idx, x in enumerate(X):\n",
    "                grad = torch.autograd.grad(selected, x,\n",
    "                                           retain_graph=True if idx + 1 < len(X) else None,\n",
    "                                           allow_unused=True)[0]\n",
    "                if grad is not None:\n",
    "                    grad = grad.cpu().numpy()\n",
    "                else:\n",
    "                    grad = torch.zeros_like(X[idx]).cpu().numpy()\n",
    "                grads.append(grad)\n",
    "            return grads\n",
    "\n",
    "    def shap_values(self, X1, X2, ranked_outputs=None, output_rank_order=\"max\", check_additivity=False):\n",
    "\n",
    "        # X ~ self.model_input\n",
    "        # X_data ~ self.data\n",
    "        \n",
    "        #X = list(load_data_deepshap(X, dtype))\n",
    "        X = list(load_data_deepshap(X1, X2, dtype))\n",
    "\n",
    "        # check if we have multiple inputs\n",
    "        if not self.multi_input:\n",
    "            assert type(X) != list, \"Expected a single tensor model input!\"\n",
    "            X = [X]\n",
    "        else:\n",
    "            assert type(X) == list, \"Expected a list of model inputs!\"\n",
    "\n",
    "\n",
    "        X = [x.detach().to(self.device) for x in X]\n",
    "\n",
    "        # if ranked output is given then this code is run and only the 'max' value given is explained\n",
    "        if ranked_outputs is not None and self.multi_output:\n",
    "            with torch.no_grad():\n",
    "                model_output_values = self.model(*X)\n",
    "                #self.model.e_fc1.weight.data = self.model.e_fc1.weight.data.mul(self.model.pathway_mask)\n",
    "                #model_output_values = self.model.relu(self.model.e_bn1(self.model.e_fc1(X[0])))\n",
    "                \n",
    "                #Whithnell's change to adjust for the additional outputs in VAE model\n",
    "                model_output_values = model_output_values[self.outputNum]\n",
    "\n",
    "            # rank and determine the model outputs that we will explain\n",
    "\n",
    "            if output_rank_order == \"max\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=True)\n",
    "            elif output_rank_order == \"min\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=False)\n",
    "            elif output_rank_order == \"max_abs\":\n",
    "                _, model_output_ranks = torch.sort(torch.abs(model_output_values), descending=True)\n",
    "            else:\n",
    "                assert False, \"output_rank_order must be max, min, or max_abs!\"\n",
    "\n",
    "        else:\n",
    "            # outputs an array of 0s so we know we are explaining the first value\n",
    "            model_output_ranks = (torch.ones((X[0].shape[0], self.num_outputs)).int() *\n",
    "                                  torch.arange(0, self.num_outputs).int())\n",
    "\n",
    "        # add the gradient handles\n",
    "\n",
    "        handles = self.add_handles(self.model, add_interim_values, deeplift_grad)\n",
    "        if self.interim:\n",
    "            self.add_target_handle(self.layer)\n",
    "\n",
    "        # compute the attributions\n",
    "        output_phis = []\n",
    "\n",
    "        for i in range(model_output_ranks.shape[1]):\n",
    "\n",
    "            phis = []\n",
    "            #phis are shapLundberg values\n",
    "\n",
    "            if self.interim:\n",
    "                for k in range(len(self.interim_inputs_shape)):\n",
    "                    phis.append(np.zeros((X[0].shape[0], ) + self.interim_inputs_shape[k][1: ]))\n",
    "            else:\n",
    "                for k in range(len(X)):\n",
    "                    phis.append(np.zeros(X[k].shape))\n",
    "            #shape is 5 as testing 5 samples\n",
    "            for j in range(X[0].shape[0]):\n",
    "\n",
    "                # tile the inputs to line up with the background data samples\n",
    "                tiled_X = [X[l][j:j + 1].repeat(\n",
    "                                   (self.data[l].shape[0],) + tuple([1 for k in range(len(X[l].shape) - 1)])) for l\n",
    "                           in range(len(X))]\n",
    "                joint_x = [torch.cat((tiled_X[l], self.data[l]), dim=0) for l in range(len(X))]\n",
    "                # run attribution computation graph\n",
    "                feature_ind = model_output_ranks[j, i]\n",
    "                sample_phis = self.gradient(feature_ind, joint_x)\n",
    "                # assign the attributions to the right part of the output arrays\n",
    "                if self.interim:\n",
    "                    sample_phis, output = sample_phis\n",
    "                    x, data = [], []\n",
    "                    for i in range(len(output)):\n",
    "                        x_temp, data_temp = np.split(output[i], 2)\n",
    "                        x.append(x_temp)\n",
    "                        data.append(data_temp)\n",
    "                    for l in range(len(self.interim_inputs_shape)):\n",
    "                        phis[l][j] = (sample_phis[l][self.data[l].shape[0]:] * (x[l] - data[l])).mean(0)\n",
    "                else:\n",
    "                    for l in range(len(X)):\n",
    "                        phis[l][j] = (torch.from_numpy(sample_phis[l][self.data[l].shape[0]:]).to(self.device) * (X[l][j: j + 1] - self.data[l])).cpu().numpy().mean(0)\n",
    "            output_phis.append(phis[0] if not self.multi_input else phis)\n",
    "\n",
    "\n",
    "        # cleanup; remove all gradient handles\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "        self.remove_attributes(self.model)\n",
    "        if self.interim:\n",
    "            self.target_handle.remove()\n",
    "\n",
    "        if not self.multi_output:\n",
    "            return output_phis[0]\n",
    "        elif ranked_outputs is not None:\n",
    "            # EW: returns a list... only want first value\n",
    "            return output_phis, model_output_ranks\n",
    "        else:\n",
    "            return output_phis\n",
    "\n",
    "# Module hooks\n",
    "\n",
    "\n",
    "def deeplift_grad(module, grad_input, grad_output):\n",
    "    \"\"\"The backward hook which computes the deeplift\n",
    "    gradient for an nn.Module\n",
    "    \"\"\"\n",
    "    # first, get the module type\n",
    "    module_type = module.__class__.__name__\n",
    "\n",
    "    # first, check the module is supported\n",
    "    if module_type in op_handler:\n",
    "\n",
    "        if op_handler[module_type].__name__ not in ['passthrough', 'linear_1d']:\n",
    "            return op_handler[module_type](module, grad_input, grad_output)\n",
    "    else:\n",
    "        print('Warning: unrecognized nn.Module: {}'.format(module_type))\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "def add_interim_values(module, input, output):\n",
    "    \"\"\"The forward hook used to save interim tensors, detached\n",
    "    from the graph. Used to calculate the multipliers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del module.x\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    try:\n",
    "        del module.y\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    module_type = module.__class__.__name__\n",
    "\n",
    "    if module_type in op_handler:\n",
    "        func_name = op_handler[module_type].__name__\n",
    "\n",
    "        # First, check for cases where we don't need to save the x and y tensors\n",
    "        if func_name == 'passthrough':\n",
    "            pass\n",
    "        else:\n",
    "            # check only the 0th input varies\n",
    "            for i in range(len(input)):\n",
    "                if i != 0 and type(output) is tuple:\n",
    "                    assert input[i] == output[i], \"Only the 0th input may vary!\"\n",
    "            # if a new method is added, it must be added here too. This ensures tensors\n",
    "            # are only saved if necessary\n",
    "            if func_name in ['maxpool', 'nonlinear_1d']:\n",
    "                # only save tensors if necessary\n",
    "                if type(input) is tuple:\n",
    "                    setattr(module, 'x', torch.nn.Parameter(input[0].detach()))\n",
    "                else:\n",
    "                    setattr(module, 'x', torch.nn.Parameter(input.detach()))\n",
    "                if type(output) is tuple:\n",
    "                    setattr(module, 'y', torch.nn.Parameter(output[0].detach()))\n",
    "                else:\n",
    "                    setattr(module, 'y', torch.nn.Parameter(output.detach()))\n",
    "            if module_type in failure_case_modules:\n",
    "                input[0].register_hook(deeplift_tensor_grad)\n",
    "\n",
    "\n",
    "def get_target_input(module, input, output):\n",
    "    \"\"\"A forward hook which saves the tensor - attached to its graph.\n",
    "    Used if we want to explain the interim outputs of a model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del module.target_input\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    setattr(module, 'target_input', input)\n",
    "\n",
    "# Whithnell:\n",
    "# From the documentation: \"The current implementation will not have the presented behavior for\n",
    "# complex Module that perform many operations. In some failure cases, grad_input and grad_output\n",
    "# will only contain the gradients for a subset of the inputs and outputs.\n",
    "# The tensor hook below handles such failure cases (currently, MaxPool1d). In such cases, the deeplift\n",
    "# grad should still be computed, and then appended to the complex_model_gradients list. The tensor hook\n",
    "# will then retrieve the proper gradient from this list.\n",
    "\n",
    "\n",
    "failure_case_modules = ['MaxPool1d']\n",
    "\n",
    "\n",
    "def deeplift_tensor_grad(grad):\n",
    "    return_grad = complex_module_gradients[-1]\n",
    "    del complex_module_gradients[-1]\n",
    "    return return_grad\n",
    "\n",
    "\n",
    "complex_module_gradients = []\n",
    "\n",
    "\n",
    "def passthrough(module, grad_input, grad_output):\n",
    "    \"\"\"No change made to gradients\"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def maxpool(module, grad_input, grad_output):\n",
    "    pool_to_unpool = {\n",
    "        'MaxPool1d': torch.nn.functional.max_unpool1d,\n",
    "        'MaxPool2d': torch.nn.functional.max_unpool2d,\n",
    "        'MaxPool3d': torch.nn.functional.max_unpool3d\n",
    "    }\n",
    "    pool_to_function = {\n",
    "        'MaxPool1d': torch.nn.functional.max_pool1d,\n",
    "        'MaxPool2d': torch.nn.functional.max_pool2d,\n",
    "        'MaxPool3d': torch.nn.functional.max_pool3d\n",
    "    }\n",
    "    delta_in = module.x[: int(module.x.shape[0] / 2)] - module.x[int(module.x.shape[0] / 2):]\n",
    "    dup0 = [2] + [1 for i in delta_in.shape[1:]]\n",
    "    # we also need to check if the output is a tuple\n",
    "    y, ref_output = torch.chunk(module.y, 2)\n",
    "    cross_max = torch.max(y, ref_output)\n",
    "    diffs = torch.cat([cross_max - ref_output, y - cross_max], 0)\n",
    "\n",
    "    # all of this just to unpool the outputs\n",
    "    with torch.no_grad():\n",
    "        _, indices = pool_to_function[module.__class__.__name__](\n",
    "            module.x, module.kernel_size, module.stride, module.padding,\n",
    "            module.dilation, module.ceil_mode, True)\n",
    "        xmax_pos, rmax_pos = torch.chunk(pool_to_unpool[module.__class__.__name__](\n",
    "            grad_output[0] * diffs, indices, module.kernel_size, module.stride,\n",
    "            module.padding, list(module.x.shape)), 2)\n",
    "    org_input_shape = grad_input[0].shape  # for the maxpool 1d\n",
    "    grad_input = [None for _ in grad_input]\n",
    "    grad_input[0] = torch.where(torch.abs(delta_in) < 1e-7, torch.zeros_like(delta_in),\n",
    "                           (xmax_pos + rmax_pos) / delta_in).repeat(dup0)\n",
    "    if module.__class__.__name__ == 'MaxPool1d':\n",
    "        complex_module_gradients.append(grad_input[0])\n",
    "        # the grad input that is returned doesn't matter, since it will immediately be\n",
    "        # be overridden by the grad in the complex_module_gradient\n",
    "        grad_input[0] = torch.ones(org_input_shape)\n",
    "    return tuple(grad_input)\n",
    "\n",
    "\n",
    "def linear_1d(module, grad_input, grad_output):\n",
    "    \"\"\"No change made to gradients.\"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def nonlinear_1d(module, grad_input, grad_output):\n",
    "    delta_out = module.y[: int(module.y.shape[0] / 2)] - module.y[int(module.y.shape[0] / 2):]\n",
    "\n",
    "    delta_in = module.x[: int(module.x.shape[0] / 2)] - module.x[int(module.x.shape[0] / 2):]\n",
    "    dup0 = [2] + [1 for i in delta_in.shape[1:]]\n",
    "    # handles numerical instabilities where delta_in is very small by\n",
    "    # just taking the gradient in those cases\n",
    "    grads = [None for _ in grad_input]\n",
    "    grads[0] = torch.where(torch.abs(delta_in.repeat(dup0)) < 1e-6, grad_input[0],\n",
    "                           grad_output[0] * (delta_out / delta_in).repeat(dup0))\n",
    "    return tuple(grads)\n",
    "\n",
    "\n",
    "op_handler = {}\n",
    "\n",
    "# passthrough ops, where we make no change to the gradient\n",
    "op_handler['Dropout3d'] = passthrough\n",
    "op_handler['Dropout2d'] = passthrough\n",
    "op_handler['Dropout'] = passthrough\n",
    "op_handler['AlphaDropout'] = passthrough\n",
    "\n",
    "op_handler['Conv1d'] = linear_1d\n",
    "op_handler['Conv2d'] = linear_1d\n",
    "op_handler['Conv3d'] = linear_1d\n",
    "op_handler['ConvTranspose1d'] = linear_1d\n",
    "op_handler['ConvTranspose2d'] = linear_1d\n",
    "op_handler['ConvTranspose3d'] = linear_1d\n",
    "op_handler['Linear'] = linear_1d\n",
    "op_handler['AvgPool1d'] = linear_1d\n",
    "op_handler['AvgPool2d'] = linear_1d\n",
    "op_handler['AvgPool3d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool1d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool2d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool3d'] = linear_1d\n",
    "op_handler['BatchNorm1d'] = linear_1d\n",
    "op_handler['BatchNorm2d'] = linear_1d\n",
    "op_handler['BatchNorm3d'] = linear_1d\n",
    "\n",
    "op_handler['LeakyReLU'] = nonlinear_1d\n",
    "op_handler['ReLU'] = nonlinear_1d\n",
    "op_handler['ELU'] = nonlinear_1d\n",
    "op_handler['Sigmoid'] = nonlinear_1d\n",
    "op_handler[\"Tanh\"] = nonlinear_1d\n",
    "op_handler[\"Softplus\"] = nonlinear_1d\n",
    "op_handler['Softmax'] = nonlinear_1d\n",
    "\n",
    "op_handler['MaxPool1d'] = maxpool\n",
    "op_handler['MaxPool2d'] = maxpool\n",
    "op_handler['MaxPool3d'] = maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e6e82a8-0952-4250-8ecc-e29a5bc30f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The sampling procedure is modified for interpreting KL_PMVAE\n",
    "\"\"\"\n",
    "\n",
    "def splitExprandSample(condition_data, sampleSize, expr):\n",
    "    match_id = get_match_id(expr, condition_data)\n",
    "    split_expr = expr.iloc[match_id]\n",
    "    split_expr.index = range(0, split_expr.shape[0], 1)\n",
    "    split_expr = split_expr.T\n",
    "    split_expr = split_expr.sample(n=sampleSize, axis=1).T\n",
    "    return split_expr\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Specify if you want to obtain importance scores for genes/miRNA (i.e., input factors) (imp_feature = True, imp_mask = False) \n",
    "or pathways (imp_feature = False, imp_mask = True)\n",
    "\"\"\"\n",
    "\n",
    "def getTopShapValues(shap_vals, numberOfTopFeatures, path, file_count, featureNames, maskNames = None, imp_feature = True, imp_mask = False, absolute=True):\n",
    "    multiple_input = False\n",
    "    if type(shap_vals) == list:\n",
    "        multiple_input = True\n",
    "        shap_values = None\n",
    "        for l in range(len(shap_vals)):\n",
    "            if shap_values is not None:\n",
    "                shap_values = np.concatenate((shap_values, shap_vals[l]), axis=1)\n",
    "            else:\n",
    "                shap_values = shap_vals[l]\n",
    "        shap_vals = shap_values\n",
    "    \n",
    "    if absolute:\n",
    "        vals = np.abs(shap_vals).mean(0)\n",
    "    else:\n",
    "        vals = shap_vals.mean(0)\n",
    "    \n",
    "    if imp_feature:\n",
    "        feature_names = featureNames\n",
    "    if imp_mask:\n",
    "        feature_names = maskNames\n",
    "\n",
    "    feature_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                      columns=['features', 'importance_vals'])\n",
    "    feature_importance.sort_values(by=['importance_vals'], ascending=False, inplace=True)\n",
    "\n",
    "    mostImp_shap_values = feature_importance.head(numberOfTopFeatures)\n",
    "    print(mostImp_shap_values)\n",
    "\n",
    "    feature_importance.to_csv(path + \"/unsup_feature_imp_\" + file_count + \"_input_all_top6.csv\")\n",
    "    #feature_importance.to_csv(path + \"/unsup_feature_imp_\" + file_count + \"_1st_hidden_all_top6.csv\")\n",
    "    #feature_importance.to_csv(path + \"/unsup_feature_imp_specific_pathway_\" + file_count + \"_top6.csv\")\n",
    "    \"\"\"\n",
    "    print(mostImp_shap_values)\n",
    "    print(\"least importance absolute values\")\n",
    "    feature_importance.sort_values(by=['feature_importance_vals'], ascending=True, inplace=True)\n",
    "    leastImp_shap_values = feature_importance.head(numberOfTopFeatures)\n",
    "    print(leastImp_shap_values)\n",
    "    \"\"\"\n",
    "    return mostImp_shap_values\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def UnSupShapExplainer(train_overall_df_gene, train_overall_df_mirna, condition_data1, condition_data2, path, file_count, dimension, featureNames, maskNames, imp_feature, imp_mask):\n",
    "    KL_PMVAE_model = KL_PMVAE_2omics(z_dim, input_n1, input_n2, pathway_mask_int)\n",
    "    \n",
    "    #load trained model\n",
    "    KL_PMVAE_model.load_state_dict(torch.load('saved_models/unsup_checkpoint_overall.pt', map_location=torch.device('cpu')))\n",
    "    \n",
    "    upper_group_sample_gene = splitExprandSample(condition_data=condition_data1, sampleSize=100, expr=train_overall_df_gene)\n",
    "    upper_mirna_match_id = get_match_id(train_overall_df_mirna, upper_group_sample_gene)\n",
    "    upper_group_sample_mirna = train_overall_df_mirna.iloc[upper_mirna_match_id]\n",
    "    \n",
    "    lower_group_sample_gene = splitExprandSample(condition_data=condition_data2, sampleSize=100, expr=train_overall_df_gene)\n",
    "    lower_mirna_match_id = get_match_id(train_overall_df_mirna, lower_group_sample_gene)\n",
    "    lower_group_sample_mirna = train_overall_df_mirna.iloc[lower_mirna_match_id]\n",
    "    \n",
    "    if imp_feature:\n",
    "        e = PyTorchDeepExplainer(KL_PMVAE_model, lower_group_sample_gene, lower_group_sample_mirna, outputNumber=0, dim=dimension, explainLatentSpace=True)\n",
    "    if imp_mask:\n",
    "        e = PyTorchDeepExplainer((KL_PMVAE_model, KL_PMVAE_model.e_fc2_mean), lower_group_sample_gene, lower_group_sample_mirna, outputNumber=0, dim=dimension, explainLatentSpace=True)\n",
    "    print(\"calculating shap values\")\n",
    "    shap_values_obtained = e.shap_values(upper_group_sample_gene, upper_group_sample_mirna)\n",
    "    \n",
    "    print(\"calculated shap values\")\n",
    "    most_imp  = getTopShapValues(shap_vals=shap_values_obtained, numberOfTopFeatures=10, path=path, file_count=file_count, featureNames=featureNames, maskNames=maskNames, imp_feature=imp_feature, imp_mask=imp_mask, absolute=True)\n",
    "    \n",
    "    return most_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a91930d6-2fd9-4889-9933-9d70d94bc325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent dimension: Z_5.\n",
      "4\n",
      "Latent dimension: Z_2.\n",
      "1\n",
      "Latent dimension: Z_4.\n",
      "3\n",
      "Latent dimension: Z_7.\n",
      "6\n",
      "Latent dimension: Z_1.\n",
      "0\n",
      "Latent dimension: Z_3.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "important_latent = [\"Z_5\", \"Z_2\", \"Z_4\", \"Z_7\", \"Z_1\", \"Z_3\"]\n",
    "#z_count = np.array(list(range(1, 17, 1))).astype('str')\n",
    "#important_latent = np.char.add('Z_', z_count).tolist()\n",
    "imp_features_cache = []\n",
    "for i in important_latent:\n",
    "    print(\"Latent dimension: \" + i + \".\")\n",
    "    temp_dim = int(i.split(\"_\")[1])-1\n",
    "    print(temp_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96fa5f2e-b43d-415b-9f68-750569410b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent dimension: Z_5.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "          features  importance_vals\n",
      "268   R-HSA-202433         0.035800\n",
      "88    R-HSA-202733         0.029402\n",
      "575  R-HSA-1251985         0.026017\n",
      "395   R-HSA-909733         0.022875\n",
      "113   R-HSA-416482         0.021857\n",
      "52    R-HSA-380108         0.020080\n",
      "310  R-HSA-1660662         0.019642\n",
      "136  R-HSA-4419969         0.019482\n",
      "215   R-HSA-983189         0.018323\n",
      "569  R-HSA-5099900         0.018070\n",
      "Latent dimension: Z_2.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "          features  importance_vals\n",
      "276   R-HSA-163560         0.083095\n",
      "312   R-HSA-174800         0.034959\n",
      "118   R-HSA-418597         0.031669\n",
      "294   R-HSA-174184         0.029305\n",
      "142   R-HSA-179409         0.028583\n",
      "132   R-HSA-381340         0.028461\n",
      "94    R-HSA-456926         0.027949\n",
      "133   R-HSA-422085         0.027234\n",
      "123   R-HSA-114608         0.026899\n",
      "433  R-HSA-1592389         0.026782\n",
      "Latent dimension: Z_4.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "          features  importance_vals\n",
      "514  R-HSA-3000170         0.048466\n",
      "4    R-HSA-1442490         0.035791\n",
      "330  R-HSA-3560783         0.035569\n",
      "548   R-HSA-354192         0.033865\n",
      "424  R-HSA-3000157         0.032647\n",
      "307  R-HSA-5687128         0.030165\n",
      "319  R-HSA-1650814         0.028240\n",
      "43     R-HSA-72163         0.027435\n",
      "88    R-HSA-202733         0.027360\n",
      "355  R-HSA-1566948         0.023435\n",
      "Latent dimension: Z_7.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "          features  importance_vals\n",
      "215   R-HSA-983189         0.015168\n",
      "516    R-HSA-69205         0.013832\n",
      "325  R-HSA-5663220         0.013042\n",
      "26    R-HSA-381038         0.012888\n",
      "133   R-HSA-422085         0.012476\n",
      "52    R-HSA-380108         0.012426\n",
      "479   R-HSA-156711         0.012151\n",
      "137   R-HSA-983231         0.012106\n",
      "307  R-HSA-5687128         0.011764\n",
      "414   R-HSA-418457         0.011513\n",
      "Latent dimension: Z_1.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "          features  importance_vals\n",
      "276   R-HSA-163560         0.059176\n",
      "7     R-HSA-380270         0.045200\n",
      "38   R-HSA-5693616         0.040245\n",
      "507  R-HSA-3232118         0.036279\n",
      "319  R-HSA-1650814         0.033426\n",
      "133   R-HSA-422085         0.031990\n",
      "0    R-HSA-1989781         0.030656\n",
      "278   R-HSA-193368         0.029433\n",
      "280   R-HSA-163615         0.028596\n",
      "250   R-HSA-977068         0.027582\n",
      "Latent dimension: Z_3.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "          features  importance_vals\n",
      "228  R-HSA-2871837         0.021410\n",
      "342  R-HSA-2162123         0.021093\n",
      "1     R-HSA-418594         0.020916\n",
      "455   R-HSA-912526         0.020654\n",
      "213  R-HSA-2132295         0.017485\n",
      "247  R-HSA-3769402         0.016848\n",
      "145  R-HSA-6782315         0.015391\n",
      "215   R-HSA-983189         0.015162\n",
      "144  R-HSA-3108214         0.015112\n",
      "95    R-HSA-114604         0.015091\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Specify if you want to obtain importance scores for genes/miRNA (i.e., input factors) (imp_feature = True, imp_mask = False) \n",
    "or pathways (imp_feature = False, imp_mask = True)\n",
    "\"\"\"\n",
    "\n",
    "# dim is the location of it \n",
    "\n",
    "important_latent = [\"Z_5\", \"Z_2\", \"Z_4\", \"Z_7\", \"Z_1\", \"Z_3\"]\n",
    "#z_count = np.array(list(range(1, 17, 1))).astype('str')\n",
    "#important_latent = np.char.add('Z_', z_count).tolist()\n",
    "imp_features_cache = []\n",
    "for i in important_latent:\n",
    "    print(\"Latent dimension: \" + i + \".\")\n",
    "    temp_dim = int(i.split(\"_\")[1])-1\n",
    "    temp_most_imp_fea = UnSupShapExplainer(train_overall_df_gene=train_data_gene, train_overall_df_mirna=train_data_mirna, condition_data1=upper_data, condition_data2=lower_data, \n",
    "                                           path='saved_models', file_count = i,\n",
    "                                           dimension=temp_dim, featureNames=feature_names, maskNames=mask_names, imp_feature=False, imp_mask=True)\n",
    "    imp_features_cache = imp_features_cache + list(temp_most_imp_fea.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "235a60a0-a82b-4da3-974b-44074d8e1e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent dimension: Z_5.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "2783     hsa.mir.190b         0.029334\n",
      "2763      hsa.mir.150         0.020468\n",
      "463   ENSG00000122863         0.020070\n",
      "1710  ENSG00000114013         0.018090\n",
      "2755      hsa.mir.142         0.018089\n",
      "452   ENSG00000121594         0.016608\n",
      "2770      hsa.mir.155         0.016545\n",
      "3085      hsa.mir.577         0.015942\n",
      "1361  ENSG00000010610         0.015597\n",
      "2756      hsa.mir.145         0.015565\n",
      "Latent dimension: Z_2.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "2800      hsa.mir.210         0.025217\n",
      "84    ENSG00000061273         0.022739\n",
      "2725    hsa.mir.1245a         0.022570\n",
      "2787     hsa.mir.193b         0.022329\n",
      "2799       hsa.mir.21         0.022184\n",
      "1807  ENSG00000123500         0.021390\n",
      "2138  ENSG00000154767         0.020025\n",
      "393   ENSG00000115414         0.019116\n",
      "1027  ENSG00000168970         0.018623\n",
      "840   ENSG00000155561         0.018165\n",
      "Latent dimension: Z_4.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "1428  ENSG00000067900         0.017148\n",
      "2164  ENSG00000156970         0.016616\n",
      "2008  ENSG00000140396         0.016213\n",
      "3063   hsa.mir.550a.3         0.014666\n",
      "2812      hsa.mir.23b         0.014542\n",
      "2433  ENSG00000177200         0.014410\n",
      "2363  ENSG00000170312         0.014313\n",
      "1705  ENSG00000113569         0.013578\n",
      "2879      hsa.mir.33b         0.013528\n",
      "840   ENSG00000155561         0.013397\n",
      "Latent dimension: Z_7.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "84    ENSG00000061273         0.024442\n",
      "2799       hsa.mir.21         0.024405\n",
      "2822      hsa.mir.29c         0.023777\n",
      "987   ENSG00000166851         0.021162\n",
      "2820    hsa.mir.29b.1         0.020858\n",
      "2754      hsa.mir.139         0.020743\n",
      "2960      hsa.mir.452         0.020729\n",
      "2877      hsa.mir.339         0.020647\n",
      "2821    hsa.mir.29b.2         0.019467\n",
      "2832       hsa.mir.31         0.019381\n",
      "Latent dimension: Z_1.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "984   ENSG00000166819         0.015541\n",
      "1095  ENSG00000175063         0.015271\n",
      "2754      hsa.mir.139         0.015151\n",
      "742   ENSG00000145386         0.013450\n",
      "152   ENSG00000079435         0.013430\n",
      "2938     hsa.mir.4326         0.013280\n",
      "3198      hsa.mir.877         0.013247\n",
      "3085      hsa.mir.577         0.012391\n",
      "2800      hsa.mir.210         0.011898\n",
      "1948  ENSG00000136158         0.011693\n",
      "Latent dimension: Z_3.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "2754      hsa.mir.139         0.044185\n",
      "984   ENSG00000166819         0.042060\n",
      "152   ENSG00000079435         0.032572\n",
      "1621  ENSG00000105974         0.031192\n",
      "2796      hsa.mir.205         0.029105\n",
      "1095  ENSG00000175063         0.027875\n",
      "2880      hsa.mir.342         0.026437\n",
      "2966      hsa.mir.455         0.025083\n",
      "1557  ENSG00000101412         0.024557\n",
      "2957     hsa.mir.4501         0.024346\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "identify key input factors (KIFs) (i.e., genes/miRNAs) for the important latent features;\n",
    "important latent features were found by applying DeepSHAP to trained LFSurv\n",
    "\"\"\"\n",
    "# dim is the location of it \n",
    "\n",
    "important_latent = [\"Z_5\", \"Z_2\", \"Z_4\", \"Z_7\", \"Z_1\", \"Z_3\"]\n",
    "#z_count = np.array(list(range(1, 17, 1))).astype('str')\n",
    "#important_latent = np.char.add('Z_', z_count).tolist()\n",
    "imp_features_cache = []\n",
    "for i in important_latent:\n",
    "    print(\"Latent dimension: \" + i + \".\")\n",
    "    temp_dim = int(i.split(\"_\")[1])-1\n",
    "    temp_most_imp_fea = UnSupShapExplainer(train_overall_df_gene=train_data_gene, train_overall_df_mirna=train_data_mirna, condition_data1=upper_data, condition_data2=lower_data, \n",
    "                                           path='saved_models', file_count = i,\n",
    "                                           dimension=temp_dim, featureNames=feature_names, maskNames=mask_names, imp_feature=True, imp_mask=False)\n",
    "    imp_features_cache = imp_features_cache + list(temp_most_imp_fea.features)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "freq = {} \n",
    "for item in imp_features_cache: \n",
    "    if (item in freq): \n",
    "        freq[item] += 1\n",
    "    else: \n",
    "        freq[item] = 1\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "freq_summary = pd.DataFrame.from_dict(freq, orient ='index')\n",
    "freq_summary.columns = ['Frequency']\n",
    "freq_summary.sort_values(by=['Frequency'], ascending=False, inplace=True)\n",
    "big_freq_summary = freq_summary[freq_summary['Frequency'] > 1]\n",
    "\n",
    "big_freq_summary.to_csv(\"saved_models/top6Z_over1_impinput.csv\")\n",
    "#big_freq_summary.to_csv(\"saved_models/top6Z_over1_imp1sthidden.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e949f874-06d3-4735-b365-a2dab4119d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DeepSHAP\n",
    "modified version\n",
    "\"\"\"\n",
    "\n",
    "class Explainer(object):\n",
    "    \"\"\" This is the superclass of all explainers.\n",
    "    \"\"\"\n",
    "\n",
    "    def shap_values(self, X):\n",
    "        raise Exception(\"SHAP values not implemented for this explainer!\")\n",
    "\n",
    "    def attributions(self, X):\n",
    "        return self.shap_values(X)\n",
    "\n",
    "class PyTorchDeepExplainer(Explainer):\n",
    "    \"\"\"\n",
    "    This class has been adapted to explain OmiVAE. It is important that the correct output of the model\n",
    "    (0=z dimension, 2=mean) as well as the dimension within this latent space. We allow a dimension to be chosen to explain from, and adjustable outputs to be explained. \n",
    "    Lundberg et al., 2017: http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data1, data2, outputNumber, dim, explainLatentSpace):\n",
    "        \n",
    "        #data = list(load_data_deepshap(data, dtype))\n",
    "        data = list(load_data_deepshap(data1, data2, dtype))\n",
    "        \n",
    "        # check if we have multiple inputs\n",
    "        self.multi_input = False\n",
    "        if type(data) == list:\n",
    "            self.multi_input = True\n",
    "        else:\n",
    "            data = [data]\n",
    "        self.data = data\n",
    "        self.layer = None\n",
    "        self.input_handle = None\n",
    "        self.interim = False\n",
    "        self.interim_inputs_shape = None\n",
    "        self.expected_value = None  # to keep the DeepExplainer base happy\n",
    "        if type(model) == tuple:\n",
    "\n",
    "            self.interim = True\n",
    "            model, layer = model\n",
    "            model = model.eval()\n",
    "            self.layer = layer\n",
    "            self.add_target_handle(self.layer)\n",
    "\n",
    "            # if we are taking an interim layer, the 'data' is going to be the input\n",
    "            # of the interim layer; we will capture this using a forward hook\n",
    "            with torch.no_grad():\n",
    "                # _ = model(*data)\n",
    "                model.e_fc1.weight.data = model.e_fc1.weight.data.mul(model.pathway_mask)\n",
    "                _ = model.relu(model.e_bn1(model.e_fc1(data[0])))\n",
    "                interim_inputs = self.layer.target_input\n",
    "                print(\"Interim input type: %s.\" %type(interim_inputs))\n",
    "                if type(interim_inputs) is tuple:\n",
    "                    # this should always be true, but just to be safe\n",
    "                    self.interim_inputs_shape = [i.shape for i in interim_inputs]\n",
    "                else:\n",
    "                    self.interim_inputs_shape = [interim_inputs.shape]\n",
    "            self.target_handle.remove()\n",
    "            del self.layer.target_input\n",
    "        self.model = model.eval()\n",
    "        self.multi_output = False\n",
    "        self.num_outputs = 1\n",
    "        with torch.no_grad():\n",
    "            # outputs = model(*data)\n",
    "            model.e_fc1.weight.data = model.e_fc1.weight.data.mul(model.pathway_mask)\n",
    "            outputs = model.relu(model.e_bn1(model.e_fc1(data[0])))\n",
    "            #This is where specifies whether we want to explain the mean or z output\n",
    "            if type(outputs) != tuple:\n",
    "                output = outputs\n",
    "            else:\n",
    "                output = outputs[outputNumber]\n",
    "            self.outputNum=outputNumber\n",
    "            # Chosen dimension\n",
    "            self.dim=None\n",
    "            self.explainLatent = False\n",
    "            if explainLatentSpace:\n",
    "                self.explainLatent=True\n",
    "                self.dimension=dim\n",
    "                output = output[:, dim]\n",
    "                output = output.reshape(output.shape[0], 1)\n",
    "            # also get the device everything is running on\n",
    "            self.device = output.device\n",
    "            if output.shape[1] > 1:\n",
    "                self.multi_output = True\n",
    "                self.num_outputs = output.shape[1]\n",
    "            self.expected_value = output.mean(0).cpu().numpy()\n",
    "\n",
    "    def add_target_handle(self, layer):\n",
    "\n",
    "        input_handle = layer.register_forward_hook(get_target_input)\n",
    "        self.target_handle = input_handle\n",
    "\n",
    "    def add_handles(self, model, forward_handle, backward_handle):\n",
    "        \"\"\"\n",
    "        Add handles to all non-container layers in the model.\n",
    "        Recursively for non-container layers\n",
    "        \"\"\"\n",
    "        handles_list = []\n",
    "        model_children = list(model.children())\n",
    "        if model_children:\n",
    "            for child in model_children:\n",
    "                handles_list.extend(self.add_handles(child, forward_handle, backward_handle))\n",
    "        else:  # leaves\n",
    "            handles_list.append(model.register_forward_hook(forward_handle))\n",
    "            handles_list.append(model.register_backward_hook(backward_handle))\n",
    "\n",
    "        return handles_list\n",
    "\n",
    "    def remove_attributes(self, model):\n",
    "        \"\"\"\n",
    "        Removes the x and y attributes which were added by the forward handles\n",
    "        Recursively searches for non-container layers\n",
    "        \"\"\"\n",
    "        for child in model.children():\n",
    "            if 'nn.modules.container' in str(type(child)):\n",
    "                self.remove_attributes(child)\n",
    "            else:\n",
    "                try:\n",
    "                    del child.x\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                try:\n",
    "                    del child.y\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "    def gradient(self, idx, inputs):\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        X = [x.requires_grad_() for x in inputs]\n",
    "        \n",
    "        # output = self.model(*X)\n",
    "        self.model.e_fc1.weight.data = self.model.e_fc1.weight.data.mul(self.model.pathway_mask)\n",
    "        output = self.model.relu(self.model.e_bn1(self.model.e_fc1(X[0])))\n",
    "\n",
    "        #Specify the output to change\n",
    "        if type(output) != tuple:\n",
    "            outputs = output\n",
    "        else:\n",
    "            outputs = output[self.outputNum]\n",
    "\n",
    "        #Specify the dimension to explain\n",
    "        if self.explainLatent==True:\n",
    "\n",
    "            outputs = outputs[:, self.dimension]\n",
    "            outputs = outputs.reshape(outputs.shape[0], 1)\n",
    "\n",
    "\n",
    "        selected = [val for val in outputs[:, idx]]\n",
    "\n",
    "        grads = []\n",
    "        if self.interim:\n",
    "            interim_inputs = self.layer.target_input\n",
    "            for idx, input in enumerate(interim_inputs):\n",
    "                grad = torch.autograd.grad(selected, input,\n",
    "                                           retain_graph=True if idx + 1 < len(interim_inputs) else None,\n",
    "                                           allow_unused=True)[0]\n",
    "                if grad is not None:\n",
    "                    grad = grad.cpu().numpy()\n",
    "                else:\n",
    "                    grad = torch.zeros_like(interim_inputs[idx]).cpu().numpy()\n",
    "                grads.append(grad)\n",
    "            del self.layer.target_input\n",
    "            return grads, [i.detach().cpu().numpy() for i in interim_inputs]\n",
    "        else:\n",
    "            for idx, x in enumerate(X):\n",
    "                grad = torch.autograd.grad(selected, x,\n",
    "                                           retain_graph=True if idx + 1 < len(X) else None,\n",
    "                                           allow_unused=True)[0]\n",
    "                if grad is not None:\n",
    "                    grad = grad.cpu().numpy()\n",
    "                else:\n",
    "                    grad = torch.zeros_like(X[idx]).cpu().numpy()\n",
    "                grads.append(grad)\n",
    "            return grads\n",
    "\n",
    "    def shap_values(self, X1, X2, ranked_outputs=None, output_rank_order=\"max\", check_additivity=False):\n",
    "\n",
    "        # X ~ self.model_input\n",
    "        # X_data ~ self.data\n",
    "        \n",
    "        #X = list(load_data_deepshap(X, dtype))\n",
    "        X = list(load_data_deepshap(X1, X2, dtype))\n",
    "\n",
    "        # check if we have multiple inputs\n",
    "        if not self.multi_input:\n",
    "            assert type(X) != list, \"Expected a single tensor model input!\"\n",
    "            X = [X]\n",
    "        else:\n",
    "            assert type(X) == list, \"Expected a list of model inputs!\"\n",
    "\n",
    "\n",
    "        X = [x.detach().to(self.device) for x in X]\n",
    "\n",
    "        # if ranked output is given then this code is run and only the 'max' value given is explained\n",
    "        if ranked_outputs is not None and self.multi_output:\n",
    "            with torch.no_grad():\n",
    "                # model_output_values = self.model(*X)\n",
    "                self.model.e_fc1.weight.data = self.model.e_fc1.weight.data.mul(self.model.pathway_mask)\n",
    "                model_output_values = self.model.relu(self.model.e_bn1(self.model.e_fc1(X[0])))\n",
    "                \n",
    "                #Whithnell's change to adjust for the additional outputs in VAE model\n",
    "                model_output_values = model_output_values[self.outputNum]\n",
    "\n",
    "            # rank and determine the model outputs that we will explain\n",
    "\n",
    "            if output_rank_order == \"max\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=True)\n",
    "            elif output_rank_order == \"min\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=False)\n",
    "            elif output_rank_order == \"max_abs\":\n",
    "                _, model_output_ranks = torch.sort(torch.abs(model_output_values), descending=True)\n",
    "            else:\n",
    "                assert False, \"output_rank_order must be max, min, or max_abs!\"\n",
    "\n",
    "        else:\n",
    "            # outputs an array of 0s so we know we are explaining the first value\n",
    "            model_output_ranks = (torch.ones((X[0].shape[0], self.num_outputs)).int() *\n",
    "                                  torch.arange(0, self.num_outputs).int())\n",
    "\n",
    "        # add the gradient handles\n",
    "\n",
    "        handles = self.add_handles(self.model, add_interim_values, deeplift_grad)\n",
    "        if self.interim:\n",
    "            self.add_target_handle(self.layer)\n",
    "\n",
    "        # compute the attributions\n",
    "        output_phis = []\n",
    "\n",
    "        for i in range(model_output_ranks.shape[1]):\n",
    "\n",
    "            phis = []\n",
    "            #phis are shapLundberg values\n",
    "\n",
    "            if self.interim:\n",
    "                for k in range(len(self.interim_inputs_shape)):\n",
    "                    phis.append(np.zeros((X[0].shape[0], ) + self.interim_inputs_shape[k][1: ]))\n",
    "            else:\n",
    "                for k in range(len(X)):\n",
    "                    phis.append(np.zeros(X[k].shape))\n",
    "            #shape is 5 as testing 5 samples\n",
    "            for j in range(X[0].shape[0]):\n",
    "\n",
    "                # tile the inputs to line up with the background data samples\n",
    "                tiled_X = [X[l][j:j + 1].repeat(\n",
    "                                   (self.data[l].shape[0],) + tuple([1 for k in range(len(X[l].shape) - 1)])) for l\n",
    "                           in range(len(X))]\n",
    "                joint_x = [torch.cat((tiled_X[l], self.data[l]), dim=0) for l in range(len(X))]\n",
    "                # run attribution computation graph\n",
    "                feature_ind = model_output_ranks[j, i]\n",
    "                sample_phis = self.gradient(feature_ind, joint_x)\n",
    "                # assign the attributions to the right part of the output arrays\n",
    "                if self.interim:\n",
    "                    sample_phis, output = sample_phis\n",
    "                    x, data = [], []\n",
    "                    for i in range(len(output)):\n",
    "                        x_temp, data_temp = np.split(output[i], 2)\n",
    "                        x.append(x_temp)\n",
    "                        data.append(data_temp)\n",
    "                    for l in range(len(self.interim_inputs_shape)):\n",
    "                        phis[l][j] = (sample_phis[l][self.data[l].shape[0]:] * (x[l] - data[l])).mean(0)\n",
    "                else:\n",
    "                    for l in range(len(X)):\n",
    "                        phis[l][j] = (torch.from_numpy(sample_phis[l][self.data[l].shape[0]:]).to(self.device) * (X[l][j: j + 1] - self.data[l])).cpu().numpy().mean(0)\n",
    "            output_phis.append(phis[0] if not self.multi_input else phis)\n",
    "\n",
    "\n",
    "        # cleanup; remove all gradient handles\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "        self.remove_attributes(self.model)\n",
    "        if self.interim:\n",
    "            self.target_handle.remove()\n",
    "\n",
    "        if not self.multi_output:\n",
    "            return output_phis[0]\n",
    "        elif ranked_outputs is not None:\n",
    "            # EW: returns a list... only want first value\n",
    "            return output_phis, model_output_ranks\n",
    "        else:\n",
    "            return output_phis\n",
    "\n",
    "# Module hooks\n",
    "\n",
    "\n",
    "def deeplift_grad(module, grad_input, grad_output):\n",
    "    \"\"\"The backward hook which computes the deeplift\n",
    "    gradient for an nn.Module\n",
    "    \"\"\"\n",
    "    # first, get the module type\n",
    "    module_type = module.__class__.__name__\n",
    "\n",
    "    # first, check the module is supported\n",
    "    if module_type in op_handler:\n",
    "\n",
    "        if op_handler[module_type].__name__ not in ['passthrough', 'linear_1d']:\n",
    "            return op_handler[module_type](module, grad_input, grad_output)\n",
    "    else:\n",
    "        print('Warning: unrecognized nn.Module: {}'.format(module_type))\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "def add_interim_values(module, input, output):\n",
    "    \"\"\"The forward hook used to save interim tensors, detached\n",
    "    from the graph. Used to calculate the multipliers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del module.x\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    try:\n",
    "        del module.y\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    module_type = module.__class__.__name__\n",
    "\n",
    "    if module_type in op_handler:\n",
    "        func_name = op_handler[module_type].__name__\n",
    "\n",
    "        # First, check for cases where we don't need to save the x and y tensors\n",
    "        if func_name == 'passthrough':\n",
    "            pass\n",
    "        else:\n",
    "            # check only the 0th input varies\n",
    "            for i in range(len(input)):\n",
    "                if i != 0 and type(output) is tuple:\n",
    "                    assert input[i] == output[i], \"Only the 0th input may vary!\"\n",
    "            # if a new method is added, it must be added here too. This ensures tensors\n",
    "            # are only saved if necessary\n",
    "            if func_name in ['maxpool', 'nonlinear_1d']:\n",
    "                # only save tensors if necessary\n",
    "                if type(input) is tuple:\n",
    "                    setattr(module, 'x', torch.nn.Parameter(input[0].detach()))\n",
    "                else:\n",
    "                    setattr(module, 'x', torch.nn.Parameter(input.detach()))\n",
    "                if type(output) is tuple:\n",
    "                    setattr(module, 'y', torch.nn.Parameter(output[0].detach()))\n",
    "                else:\n",
    "                    setattr(module, 'y', torch.nn.Parameter(output.detach()))\n",
    "            if module_type in failure_case_modules:\n",
    "                input[0].register_hook(deeplift_tensor_grad)\n",
    "\n",
    "\n",
    "def get_target_input(module, input, output):\n",
    "    \"\"\"A forward hook which saves the tensor - attached to its graph.\n",
    "    Used if we want to explain the interim outputs of a model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del module.target_input\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    setattr(module, 'target_input', input)\n",
    "\n",
    "# Whithnell:\n",
    "# From the documentation: \"The current implementation will not have the presented behavior for\n",
    "# complex Module that perform many operations. In some failure cases, grad_input and grad_output\n",
    "# will only contain the gradients for a subset of the inputs and outputs.\n",
    "# The tensor hook below handles such failure cases (currently, MaxPool1d). In such cases, the deeplift\n",
    "# grad should still be computed, and then appended to the complex_model_gradients list. The tensor hook\n",
    "# will then retrieve the proper gradient from this list.\n",
    "\n",
    "\n",
    "failure_case_modules = ['MaxPool1d']\n",
    "\n",
    "\n",
    "def deeplift_tensor_grad(grad):\n",
    "    return_grad = complex_module_gradients[-1]\n",
    "    del complex_module_gradients[-1]\n",
    "    return return_grad\n",
    "\n",
    "\n",
    "complex_module_gradients = []\n",
    "\n",
    "\n",
    "def passthrough(module, grad_input, grad_output):\n",
    "    \"\"\"No change made to gradients\"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def maxpool(module, grad_input, grad_output):\n",
    "    pool_to_unpool = {\n",
    "        'MaxPool1d': torch.nn.functional.max_unpool1d,\n",
    "        'MaxPool2d': torch.nn.functional.max_unpool2d,\n",
    "        'MaxPool3d': torch.nn.functional.max_unpool3d\n",
    "    }\n",
    "    pool_to_function = {\n",
    "        'MaxPool1d': torch.nn.functional.max_pool1d,\n",
    "        'MaxPool2d': torch.nn.functional.max_pool2d,\n",
    "        'MaxPool3d': torch.nn.functional.max_pool3d\n",
    "    }\n",
    "    delta_in = module.x[: int(module.x.shape[0] / 2)] - module.x[int(module.x.shape[0] / 2):]\n",
    "    dup0 = [2] + [1 for i in delta_in.shape[1:]]\n",
    "    # we also need to check if the output is a tuple\n",
    "    y, ref_output = torch.chunk(module.y, 2)\n",
    "    cross_max = torch.max(y, ref_output)\n",
    "    diffs = torch.cat([cross_max - ref_output, y - cross_max], 0)\n",
    "\n",
    "    # all of this just to unpool the outputs\n",
    "    with torch.no_grad():\n",
    "        _, indices = pool_to_function[module.__class__.__name__](\n",
    "            module.x, module.kernel_size, module.stride, module.padding,\n",
    "            module.dilation, module.ceil_mode, True)\n",
    "        xmax_pos, rmax_pos = torch.chunk(pool_to_unpool[module.__class__.__name__](\n",
    "            grad_output[0] * diffs, indices, module.kernel_size, module.stride,\n",
    "            module.padding, list(module.x.shape)), 2)\n",
    "    org_input_shape = grad_input[0].shape  # for the maxpool 1d\n",
    "    grad_input = [None for _ in grad_input]\n",
    "    grad_input[0] = torch.where(torch.abs(delta_in) < 1e-7, torch.zeros_like(delta_in),\n",
    "                           (xmax_pos + rmax_pos) / delta_in).repeat(dup0)\n",
    "    if module.__class__.__name__ == 'MaxPool1d':\n",
    "        complex_module_gradients.append(grad_input[0])\n",
    "        # the grad input that is returned doesn't matter, since it will immediately be\n",
    "        # be overridden by the grad in the complex_module_gradient\n",
    "        grad_input[0] = torch.ones(org_input_shape)\n",
    "    return tuple(grad_input)\n",
    "\n",
    "\n",
    "def linear_1d(module, grad_input, grad_output):\n",
    "    \"\"\"No change made to gradients.\"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def nonlinear_1d(module, grad_input, grad_output):\n",
    "    delta_out = module.y[: int(module.y.shape[0] / 2)] - module.y[int(module.y.shape[0] / 2):]\n",
    "\n",
    "    delta_in = module.x[: int(module.x.shape[0] / 2)] - module.x[int(module.x.shape[0] / 2):]\n",
    "    dup0 = [2] + [1 for i in delta_in.shape[1:]]\n",
    "    # handles numerical instabilities where delta_in is very small by\n",
    "    # just taking the gradient in those cases\n",
    "    grads = [None for _ in grad_input]\n",
    "    grads[0] = torch.where(torch.abs(delta_in.repeat(dup0)) < 1e-6, grad_input[0],\n",
    "                           grad_output[0] * (delta_out / delta_in).repeat(dup0))\n",
    "    return tuple(grads)\n",
    "\n",
    "\n",
    "op_handler = {}\n",
    "\n",
    "# passthrough ops, where we make no change to the gradient\n",
    "op_handler['Dropout3d'] = passthrough\n",
    "op_handler['Dropout2d'] = passthrough\n",
    "op_handler['Dropout'] = passthrough\n",
    "op_handler['AlphaDropout'] = passthrough\n",
    "\n",
    "op_handler['Conv1d'] = linear_1d\n",
    "op_handler['Conv2d'] = linear_1d\n",
    "op_handler['Conv3d'] = linear_1d\n",
    "op_handler['ConvTranspose1d'] = linear_1d\n",
    "op_handler['ConvTranspose2d'] = linear_1d\n",
    "op_handler['ConvTranspose3d'] = linear_1d\n",
    "op_handler['Linear'] = linear_1d\n",
    "op_handler['AvgPool1d'] = linear_1d\n",
    "op_handler['AvgPool2d'] = linear_1d\n",
    "op_handler['AvgPool3d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool1d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool2d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool3d'] = linear_1d\n",
    "op_handler['BatchNorm1d'] = linear_1d\n",
    "op_handler['BatchNorm2d'] = linear_1d\n",
    "op_handler['BatchNorm3d'] = linear_1d\n",
    "\n",
    "op_handler['LeakyReLU'] = nonlinear_1d\n",
    "op_handler['ReLU'] = nonlinear_1d\n",
    "op_handler['ELU'] = nonlinear_1d\n",
    "op_handler['Sigmoid'] = nonlinear_1d\n",
    "op_handler[\"Tanh\"] = nonlinear_1d\n",
    "op_handler[\"Softplus\"] = nonlinear_1d\n",
    "op_handler['Softmax'] = nonlinear_1d\n",
    "\n",
    "op_handler['MaxPool1d'] = maxpool\n",
    "op_handler['MaxPool2d'] = maxpool\n",
    "op_handler['MaxPool3d'] = maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab56083e-079f-48db-9043-55d53f79285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cloud-home/U1039935/.magellan/conda/envs/VAE/lib/python3.12/site-packages/torch/nn/modules/module.py:1373: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "984   ENSG00000166819         0.066708\n",
      "152   ENSG00000079435         0.057203\n",
      "1621  ENSG00000105974         0.051455\n",
      "1042  ENSG00000170323         0.030984\n",
      "128   ENSG00000074416         0.027413\n",
      "711   ENSG00000142875         0.003374\n",
      "1040  ENSG00000170231         0.002437\n",
      "2158  ENSG00000156414         0.000000\n",
      "2147  ENSG00000155849         0.000000\n",
      "2141  ENSG00000155096         0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Identify important genes for important pathways\n",
    "file_count = specific pathway name we are interested.\n",
    "dimension = location of this file's order in the network/data \n",
    "\"\"\"\n",
    "\n",
    "# Remember to change model.output in PyTorchDeepExplainer() to the values of the first hidden layer before running the following command\n",
    "most_important_features = UnSupShapExplainer(train_overall_df_gene=train_data_gene, train_overall_df_mirna=train_data_mirna, condition_data1=upper_data, condition_data2=lower_data, \n",
    "                                             path='saved_models', file_count = \"R.HSA.163560\",\n",
    "                                             dimension=276, featureNames=feature_names, maskNames=mask_names, imp_feature=True, imp_mask=False)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bca826-cfd8-4ea1-a451-d12f606e15c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      R-HSA-2142691  R-HSA-181429  R-HSA-381426  R-HSA-432142\n",
      "0                 0             0             0             1\n",
      "1                 0             0             0             0\n",
      "2                 0             0             0             0\n",
      "3                 0             0             0             0\n",
      "4                 0             0             0             0\n",
      "...             ...           ...           ...           ...\n",
      "2694              0             0             0             0\n",
      "2695              0             0             0             0\n",
      "2696              0             0             0             0\n",
      "2697              0             0             0             0\n",
      "2698              0             0             0             0\n",
      "\n",
      "[2699 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"processed_data_example/TCGA_BRCA/tune/minmax_normalized/pathway_mask.csv\")\n",
    "selected_columns = df.filter(like='142', axis=1)\n",
    "\n",
    "\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf14992-a5a9-487f-99c6-b413bc623172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-HSA-2029482</th>\n",
       "      <th>R-HSA-5663213</th>\n",
       "      <th>R-HSA-201681</th>\n",
       "      <th>R-HSA-3238698</th>\n",
       "      <th>R-HSA-373080</th>\n",
       "      <th>R-HSA-4086400</th>\n",
       "      <th>R-HSA-4641262</th>\n",
       "      <th>R-HSA-163560</th>\n",
       "      <th>R-HSA-70895</th>\n",
       "      <th>R-HSA-193368</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2699 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      R-HSA-2029482  R-HSA-5663213  R-HSA-201681  R-HSA-3238698  R-HSA-373080  \\\n",
       "0                 0              0             0              0             0   \n",
       "1                 0              0             0              0             0   \n",
       "2                 0              0             0              0             0   \n",
       "3                 0              0             0              0             0   \n",
       "4                 0              0             0              0             0   \n",
       "...             ...            ...           ...            ...           ...   \n",
       "2694              0              0             0              0             0   \n",
       "2695              0              0             0              0             0   \n",
       "2696              0              0             0              0             0   \n",
       "2697              0              0             0              0             0   \n",
       "2698              0              0             0              0             0   \n",
       "\n",
       "      R-HSA-4086400  R-HSA-4641262  R-HSA-163560  R-HSA-70895  R-HSA-193368  \n",
       "0                 0              0             0            0             0  \n",
       "1                 0              0             0            0             0  \n",
       "2                 0              0             0            0             0  \n",
       "3                 0              0             0            0             0  \n",
       "4                 0              0             0            0             0  \n",
       "...             ...            ...           ...          ...           ...  \n",
       "2694              0              0             0            0             0  \n",
       "2695              0              0             0            0             0  \n",
       "2696              0              0             0            0             0  \n",
       "2697              0              0             0            0             0  \n",
       "2698              0              0             0            0             0  \n",
       "\n",
       "[2699 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 270:280]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAE",
   "language": "python",
   "name": "vae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
