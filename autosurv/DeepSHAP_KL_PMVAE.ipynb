{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65023bce-6b39-4117-b7f3-39df42d9b245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "from torch.autograd import Variable\n",
    "from KL_PMVAE_shap import KL_PMVAE_2omics\n",
    "from utils import get_match_id\n",
    "\n",
    "dtype = torch.FloatTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a112ae1e-bb22-4491-95a6-a02b558d556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def load_data_deepshap(data1, data2, dtype):\n",
    "    EXPR1 = data1.drop([\"patient_id\", \"OS\", \"OS.time\", \"age\", \"race_white\", \"stage_i\", \"stage_ii\"], axis = 1).values.astype(np.float64)\n",
    "    EXPR2 = data2.drop([\"patient_id\", \"OS\", \"OS.time\", \"age\", \"race_white\", \"stage_i\", \"stage_ii\"], axis = 1).values.astype(np.float64)\n",
    "    EXPR1 = torch.from_numpy(EXPR1).type(dtype)\n",
    "    EXPR2 = torch.from_numpy(EXPR2).type(dtype)\n",
    "        \n",
    "    return (EXPR1, EXPR2)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def load_pathway(path, dtype):\n",
    "    '''Load a bi-adjacency matrix of pathways, and then covert it to a Pytorch tensor.\n",
    "    Input:\n",
    "        path: path to input dataset (which is expected to be a csv file).\n",
    "        dtype: define the data type of tensor (i.e. dtype=torch.FloatTensor)\n",
    "    Output:\n",
    "        PATHWAY_MASK: a Pytorch tensor of the bi-adjacency matrix of pathways & genes.\n",
    "    '''\n",
    "    pathway_mask = pd.read_csv(path, index_col = 0).to_numpy()\n",
    "\n",
    "    PATHWAY_MASK = torch.from_numpy(pathway_mask).type(dtype)\n",
    "    PATHWAY_MASK = torch.transpose(PATHWAY_MASK, 0, 1)\n",
    "    \n",
    "    return(PATHWAY_MASK)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa234c3c-12f6-4289-99cc-4e3e40a74dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "train_data_gene = pd.read_csv(\"processed_data_example/TCGA_BRCA/train_test_split/minmax_normalized/data_train_gene_minmax_overall.csv\")\n",
    "train_data_mirna = pd.read_csv(\"processed_data_example/TCGA_BRCA/train_test_split/minmax_normalized/data_train_mirna_minmax_overall.csv\")\n",
    "\n",
    "gene_names = list(train_data_gene.columns)[7:]\n",
    "mirna_names = list(train_data_mirna.columns)[7:]\n",
    "feature_names = gene_names + mirna_names\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "upper_data = pd.read_csv(\"saved_models/higher_PI_train.csv\")\n",
    "lower_data = pd.read_csv(\"saved_models/lower_PI_train.csv\")\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "pathway_mask = pd.read_csv(\"processed_data_example/TCGA_BRCA/train_test_split/minmax_normalized/pathway_mask.csv\", index_col = 0)\n",
    "pathway_names = list(pathway_mask.columns)\n",
    "mask_names = pathway_names + mirna_names\n",
    "pathway_mask_int = load_pathway(\"processed_data_example/TCGA_BRCA/train_test_split/minmax_normalized/pathway_mask.csv\", dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c4f8a2a-9de3-4fe0-b4b1-d4db780ee1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Breast cancer dataset used as example here\n",
    "\"\"\"\n",
    "# number of genes\n",
    "input_n1 = 2699\n",
    "# number of miRNAs\n",
    "input_n2 = 516\n",
    "# number of latent features: one of the hyperparameters, determined via grid search during training/validation process\n",
    "z_dim = 8\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85eb9be1-b4b9-4a36-89f2-0a157481d7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DeepSHAP\n",
    "\n",
    "These codes were adapted from the work of Withnell et al. https://academic.oup.com/bib/article/22/6/bbab315/6353242\n",
    "Please check their original implementation of DeepSHAP for more details at https://github.com/zhangxiaoyu11/XOmiVAE\n",
    "\"\"\"\n",
    "\n",
    "class Explainer(object):\n",
    "    \"\"\" This is the superclass of all explainers.\n",
    "    \"\"\"\n",
    "\n",
    "    def shap_values(self, X):\n",
    "        raise Exception(\"SHAP values not implemented for this explainer!\")\n",
    "\n",
    "    def attributions(self, X):\n",
    "        return self.shap_values(X)\n",
    "\n",
    "class PyTorchDeepExplainer(Explainer):\n",
    "    \"\"\"\n",
    "    This class has been adapted to explain OmiVAE. It is important that the correct output of the model\n",
    "    (0=z dimension, 2=mean) as well as the dimension within this latent space. We allow a dimension to be chosen to explain from, and adjustable outputs to be explained. \n",
    "    Lundberg et al., 2017: http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data1, data2, outputNumber, dim, explainLatentSpace):\n",
    "        \n",
    "        #data = list(load_data_deepshap(data, dtype))\n",
    "        data = list(load_data_deepshap(data1, data2, dtype))\n",
    "        \n",
    "        # check if we have multiple inputs\n",
    "        self.multi_input = False\n",
    "        if type(data) == list:\n",
    "            self.multi_input = True\n",
    "        else:\n",
    "            data = [data]\n",
    "        self.data = data\n",
    "        self.layer = None\n",
    "        self.input_handle = None\n",
    "        self.interim = False\n",
    "        self.interim_inputs_shape = None\n",
    "        self.expected_value = None  # to keep the DeepExplainer base happy\n",
    "        if type(model) == tuple:\n",
    "\n",
    "            self.interim = True\n",
    "            model, layer = model\n",
    "            model = model.eval()\n",
    "            self.layer = layer\n",
    "            self.add_target_handle(self.layer)\n",
    "\n",
    "            # if we are taking an interim layer, the 'data' is going to be the input\n",
    "            # of the interim layer; we will capture this using a forward hook\n",
    "            with torch.no_grad():\n",
    "                _ = model(*data)\n",
    "                #model.e_fc1.weight.data = model.e_fc1.weight.data.mul(model.pathway_mask)\n",
    "                #_ = model.relu(model.e_bn1(model.e_fc1(data[0])))\n",
    "                interim_inputs = self.layer.target_input\n",
    "                #print(\"Interim input type: %s.\" %type(interim_inputs))\n",
    "                if type(interim_inputs) is tuple:\n",
    "                    # this should always be true, but just to be safe\n",
    "                    self.interim_inputs_shape = [i.shape for i in interim_inputs]\n",
    "                else:\n",
    "                    self.interim_inputs_shape = [interim_inputs.shape]\n",
    "            self.target_handle.remove()\n",
    "            del self.layer.target_input\n",
    "        self.model = model.eval()\n",
    "        self.multi_output = False\n",
    "        self.num_outputs = 1\n",
    "        with torch.no_grad():\n",
    "            outputs = model(*data)\n",
    "            #model.e_fc1.weight.data = model.e_fc1.weight.data.mul(model.pathway_mask)\n",
    "            #outputs = model.relu(model.e_bn1(model.e_fc1(data[0])))\n",
    "            #This is where specifies whether we want to explain the mean or z output\n",
    "            if type(outputs) != tuple:\n",
    "                output = outputs\n",
    "            else:\n",
    "                output = outputs[outputNumber]\n",
    "            self.outputNum=outputNumber\n",
    "            # Chosen dimension\n",
    "            self.dim=None\n",
    "            self.explainLatent = False\n",
    "            if explainLatentSpace:\n",
    "                self.explainLatent=True\n",
    "                self.dimension=dim\n",
    "                output = output[:, dim]\n",
    "                output = output.reshape(output.shape[0], 1)\n",
    "            # also get the device everything is running on\n",
    "            self.device = output.device\n",
    "            if output.shape[1] > 1:\n",
    "                self.multi_output = True\n",
    "                self.num_outputs = output.shape[1]\n",
    "            self.expected_value = output.mean(0).cpu().numpy()\n",
    "\n",
    "    def add_target_handle(self, layer):\n",
    "\n",
    "        input_handle = layer.register_forward_hook(get_target_input)\n",
    "        self.target_handle = input_handle\n",
    "\n",
    "    def add_handles(self, model, forward_handle, backward_handle):\n",
    "        \"\"\"\n",
    "        Add handles to all non-container layers in the model.\n",
    "        Recursively for non-container layers\n",
    "        \"\"\"\n",
    "        handles_list = []\n",
    "        model_children = list(model.children())\n",
    "        if model_children:\n",
    "            for child in model_children:\n",
    "                handles_list.extend(self.add_handles(child, forward_handle, backward_handle))\n",
    "        else:  # leaves\n",
    "            handles_list.append(model.register_forward_hook(forward_handle))\n",
    "            handles_list.append(model.register_backward_hook(backward_handle))\n",
    "\n",
    "        return handles_list\n",
    "\n",
    "    def remove_attributes(self, model):\n",
    "        \"\"\"\n",
    "        Removes the x and y attributes which were added by the forward handles\n",
    "        Recursively searches for non-container layers\n",
    "        \"\"\"\n",
    "        for child in model.children():\n",
    "            if 'nn.modules.container' in str(type(child)):\n",
    "                self.remove_attributes(child)\n",
    "            else:\n",
    "                try:\n",
    "                    del child.x\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                try:\n",
    "                    del child.y\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "    def gradient(self, idx, inputs):\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        X = [x.requires_grad_() for x in inputs]\n",
    "        \n",
    "        output = self.model(*X)\n",
    "        #self.model.e_fc1.weight.data = self.model.e_fc1.weight.data.mul(self.model.pathway_mask)\n",
    "        #output = self.model.relu(self.model.e_bn1(self.model.e_fc1(X[0])))\n",
    "\n",
    "        #Specify the output to change\n",
    "        if type(output) != tuple:\n",
    "            outputs = output\n",
    "        else:\n",
    "            outputs = output[self.outputNum]\n",
    "\n",
    "        #Specify the dimension to explain\n",
    "        if self.explainLatent==True:\n",
    "\n",
    "            outputs = outputs[:, self.dimension]\n",
    "            outputs = outputs.reshape(outputs.shape[0], 1)\n",
    "\n",
    "\n",
    "        selected = [val for val in outputs[:, idx]]\n",
    "\n",
    "        grads = []\n",
    "        if self.interim:\n",
    "            interim_inputs = self.layer.target_input\n",
    "            for idx, input in enumerate(interim_inputs):\n",
    "                grad = torch.autograd.grad(selected, input,\n",
    "                                           retain_graph=True if idx + 1 < len(interim_inputs) else None,\n",
    "                                           allow_unused=True)[0]\n",
    "                if grad is not None:\n",
    "                    grad = grad.cpu().numpy()\n",
    "                else:\n",
    "                    grad = torch.zeros_like(interim_inputs[idx]).cpu().numpy()\n",
    "                grads.append(grad)\n",
    "            del self.layer.target_input\n",
    "            return grads, [i.detach().cpu().numpy() for i in interim_inputs]\n",
    "        else:\n",
    "            for idx, x in enumerate(X):\n",
    "                grad = torch.autograd.grad(selected, x,\n",
    "                                           retain_graph=True if idx + 1 < len(X) else None,\n",
    "                                           allow_unused=True)[0]\n",
    "                if grad is not None:\n",
    "                    grad = grad.cpu().numpy()\n",
    "                else:\n",
    "                    grad = torch.zeros_like(X[idx]).cpu().numpy()\n",
    "                grads.append(grad)\n",
    "            return grads\n",
    "\n",
    "    def shap_values(self, X1, X2, ranked_outputs=None, output_rank_order=\"max\", check_additivity=False):\n",
    "\n",
    "        # X ~ self.model_input\n",
    "        # X_data ~ self.data\n",
    "        \n",
    "        #X = list(load_data_deepshap(X, dtype))\n",
    "        X = list(load_data_deepshap(X1, X2, dtype))\n",
    "\n",
    "        # check if we have multiple inputs\n",
    "        if not self.multi_input:\n",
    "            assert type(X) != list, \"Expected a single tensor model input!\"\n",
    "            X = [X]\n",
    "        else:\n",
    "            assert type(X) == list, \"Expected a list of model inputs!\"\n",
    "\n",
    "\n",
    "        X = [x.detach().to(self.device) for x in X]\n",
    "\n",
    "        # if ranked output is given then this code is run and only the 'max' value given is explained\n",
    "        if ranked_outputs is not None and self.multi_output:\n",
    "            with torch.no_grad():\n",
    "                model_output_values = self.model(*X)\n",
    "                #self.model.e_fc1.weight.data = self.model.e_fc1.weight.data.mul(self.model.pathway_mask)\n",
    "                #model_output_values = self.model.relu(self.model.e_bn1(self.model.e_fc1(X[0])))\n",
    "                \n",
    "                #Whithnell's change to adjust for the additional outputs in VAE model\n",
    "                model_output_values = model_output_values[self.outputNum]\n",
    "\n",
    "            # rank and determine the model outputs that we will explain\n",
    "\n",
    "            if output_rank_order == \"max\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=True)\n",
    "            elif output_rank_order == \"min\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=False)\n",
    "            elif output_rank_order == \"max_abs\":\n",
    "                _, model_output_ranks = torch.sort(torch.abs(model_output_values), descending=True)\n",
    "            else:\n",
    "                assert False, \"output_rank_order must be max, min, or max_abs!\"\n",
    "\n",
    "        else:\n",
    "            # outputs an array of 0s so we know we are explaining the first value\n",
    "            model_output_ranks = (torch.ones((X[0].shape[0], self.num_outputs)).int() *\n",
    "                                  torch.arange(0, self.num_outputs).int())\n",
    "\n",
    "        # add the gradient handles\n",
    "\n",
    "        handles = self.add_handles(self.model, add_interim_values, deeplift_grad)\n",
    "        if self.interim:\n",
    "            self.add_target_handle(self.layer)\n",
    "\n",
    "        # compute the attributions\n",
    "        output_phis = []\n",
    "\n",
    "        for i in range(model_output_ranks.shape[1]):\n",
    "\n",
    "            phis = []\n",
    "            #phis are shapLundberg values\n",
    "\n",
    "            if self.interim:\n",
    "                for k in range(len(self.interim_inputs_shape)):\n",
    "                    phis.append(np.zeros((X[0].shape[0], ) + self.interim_inputs_shape[k][1: ]))\n",
    "            else:\n",
    "                for k in range(len(X)):\n",
    "                    phis.append(np.zeros(X[k].shape))\n",
    "            #shape is 5 as testing 5 samples\n",
    "            for j in range(X[0].shape[0]):\n",
    "\n",
    "                # tile the inputs to line up with the background data samples\n",
    "                tiled_X = [X[l][j:j + 1].repeat(\n",
    "                                   (self.data[l].shape[0],) + tuple([1 for k in range(len(X[l].shape) - 1)])) for l\n",
    "                           in range(len(X))]\n",
    "                joint_x = [torch.cat((tiled_X[l], self.data[l]), dim=0) for l in range(len(X))]\n",
    "                # run attribution computation graph\n",
    "                feature_ind = model_output_ranks[j, i]\n",
    "                sample_phis = self.gradient(feature_ind, joint_x)\n",
    "                # assign the attributions to the right part of the output arrays\n",
    "                if self.interim:\n",
    "                    sample_phis, output = sample_phis\n",
    "                    x, data = [], []\n",
    "                    for i in range(len(output)):\n",
    "                        x_temp, data_temp = np.split(output[i], 2)\n",
    "                        x.append(x_temp)\n",
    "                        data.append(data_temp)\n",
    "                    for l in range(len(self.interim_inputs_shape)):\n",
    "                        phis[l][j] = (sample_phis[l][self.data[l].shape[0]:] * (x[l] - data[l])).mean(0)\n",
    "                else:\n",
    "                    for l in range(len(X)):\n",
    "                        phis[l][j] = (torch.from_numpy(sample_phis[l][self.data[l].shape[0]:]).to(self.device) * (X[l][j: j + 1] - self.data[l])).cpu().numpy().mean(0)\n",
    "            output_phis.append(phis[0] if not self.multi_input else phis)\n",
    "\n",
    "\n",
    "        # cleanup; remove all gradient handles\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "        self.remove_attributes(self.model)\n",
    "        if self.interim:\n",
    "            self.target_handle.remove()\n",
    "\n",
    "        if not self.multi_output:\n",
    "            return output_phis[0]\n",
    "        elif ranked_outputs is not None:\n",
    "            # EW: returns a list... only want first value\n",
    "            return output_phis, model_output_ranks\n",
    "        else:\n",
    "            return output_phis\n",
    "\n",
    "# Module hooks\n",
    "\n",
    "\n",
    "def deeplift_grad(module, grad_input, grad_output):\n",
    "    \"\"\"The backward hook which computes the deeplift\n",
    "    gradient for an nn.Module\n",
    "    \"\"\"\n",
    "    # first, get the module type\n",
    "    module_type = module.__class__.__name__\n",
    "\n",
    "    # first, check the module is supported\n",
    "    if module_type in op_handler:\n",
    "\n",
    "        if op_handler[module_type].__name__ not in ['passthrough', 'linear_1d']:\n",
    "            return op_handler[module_type](module, grad_input, grad_output)\n",
    "    else:\n",
    "        print('Warning: unrecognized nn.Module: {}'.format(module_type))\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "def add_interim_values(module, input, output):\n",
    "    \"\"\"The forward hook used to save interim tensors, detached\n",
    "    from the graph. Used to calculate the multipliers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del module.x\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    try:\n",
    "        del module.y\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    module_type = module.__class__.__name__\n",
    "\n",
    "    if module_type in op_handler:\n",
    "        func_name = op_handler[module_type].__name__\n",
    "\n",
    "        # First, check for cases where we don't need to save the x and y tensors\n",
    "        if func_name == 'passthrough':\n",
    "            pass\n",
    "        else:\n",
    "            # check only the 0th input varies\n",
    "            for i in range(len(input)):\n",
    "                if i != 0 and type(output) is tuple:\n",
    "                    assert input[i] == output[i], \"Only the 0th input may vary!\"\n",
    "            # if a new method is added, it must be added here too. This ensures tensors\n",
    "            # are only saved if necessary\n",
    "            if func_name in ['maxpool', 'nonlinear_1d']:\n",
    "                # only save tensors if necessary\n",
    "                if type(input) is tuple:\n",
    "                    setattr(module, 'x', torch.nn.Parameter(input[0].detach()))\n",
    "                else:\n",
    "                    setattr(module, 'x', torch.nn.Parameter(input.detach()))\n",
    "                if type(output) is tuple:\n",
    "                    setattr(module, 'y', torch.nn.Parameter(output[0].detach()))\n",
    "                else:\n",
    "                    setattr(module, 'y', torch.nn.Parameter(output.detach()))\n",
    "            if module_type in failure_case_modules:\n",
    "                input[0].register_hook(deeplift_tensor_grad)\n",
    "\n",
    "\n",
    "def get_target_input(module, input, output):\n",
    "    \"\"\"A forward hook which saves the tensor - attached to its graph.\n",
    "    Used if we want to explain the interim outputs of a model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del module.target_input\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    setattr(module, 'target_input', input)\n",
    "\n",
    "# Whithnell:\n",
    "# From the documentation: \"The current implementation will not have the presented behavior for\n",
    "# complex Module that perform many operations. In some failure cases, grad_input and grad_output\n",
    "# will only contain the gradients for a subset of the inputs and outputs.\n",
    "# The tensor hook below handles such failure cases (currently, MaxPool1d). In such cases, the deeplift\n",
    "# grad should still be computed, and then appended to the complex_model_gradients list. The tensor hook\n",
    "# will then retrieve the proper gradient from this list.\n",
    "\n",
    "\n",
    "failure_case_modules = ['MaxPool1d']\n",
    "\n",
    "\n",
    "def deeplift_tensor_grad(grad):\n",
    "    return_grad = complex_module_gradients[-1]\n",
    "    del complex_module_gradients[-1]\n",
    "    return return_grad\n",
    "\n",
    "\n",
    "complex_module_gradients = []\n",
    "\n",
    "\n",
    "def passthrough(module, grad_input, grad_output):\n",
    "    \"\"\"No change made to gradients\"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def maxpool(module, grad_input, grad_output):\n",
    "    pool_to_unpool = {\n",
    "        'MaxPool1d': torch.nn.functional.max_unpool1d,\n",
    "        'MaxPool2d': torch.nn.functional.max_unpool2d,\n",
    "        'MaxPool3d': torch.nn.functional.max_unpool3d\n",
    "    }\n",
    "    pool_to_function = {\n",
    "        'MaxPool1d': torch.nn.functional.max_pool1d,\n",
    "        'MaxPool2d': torch.nn.functional.max_pool2d,\n",
    "        'MaxPool3d': torch.nn.functional.max_pool3d\n",
    "    }\n",
    "    delta_in = module.x[: int(module.x.shape[0] / 2)] - module.x[int(module.x.shape[0] / 2):]\n",
    "    dup0 = [2] + [1 for i in delta_in.shape[1:]]\n",
    "    # we also need to check if the output is a tuple\n",
    "    y, ref_output = torch.chunk(module.y, 2)\n",
    "    cross_max = torch.max(y, ref_output)\n",
    "    diffs = torch.cat([cross_max - ref_output, y - cross_max], 0)\n",
    "\n",
    "    # all of this just to unpool the outputs\n",
    "    with torch.no_grad():\n",
    "        _, indices = pool_to_function[module.__class__.__name__](\n",
    "            module.x, module.kernel_size, module.stride, module.padding,\n",
    "            module.dilation, module.ceil_mode, True)\n",
    "        xmax_pos, rmax_pos = torch.chunk(pool_to_unpool[module.__class__.__name__](\n",
    "            grad_output[0] * diffs, indices, module.kernel_size, module.stride,\n",
    "            module.padding, list(module.x.shape)), 2)\n",
    "    org_input_shape = grad_input[0].shape  # for the maxpool 1d\n",
    "    grad_input = [None for _ in grad_input]\n",
    "    grad_input[0] = torch.where(torch.abs(delta_in) < 1e-7, torch.zeros_like(delta_in),\n",
    "                           (xmax_pos + rmax_pos) / delta_in).repeat(dup0)\n",
    "    if module.__class__.__name__ == 'MaxPool1d':\n",
    "        complex_module_gradients.append(grad_input[0])\n",
    "        # the grad input that is returned doesn't matter, since it will immediately be\n",
    "        # be overridden by the grad in the complex_module_gradient\n",
    "        grad_input[0] = torch.ones(org_input_shape)\n",
    "    return tuple(grad_input)\n",
    "\n",
    "\n",
    "def linear_1d(module, grad_input, grad_output):\n",
    "    \"\"\"No change made to gradients.\"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def nonlinear_1d(module, grad_input, grad_output):\n",
    "    delta_out = module.y[: int(module.y.shape[0] / 2)] - module.y[int(module.y.shape[0] / 2):]\n",
    "\n",
    "    delta_in = module.x[: int(module.x.shape[0] / 2)] - module.x[int(module.x.shape[0] / 2):]\n",
    "    dup0 = [2] + [1 for i in delta_in.shape[1:]]\n",
    "    # handles numerical instabilities where delta_in is very small by\n",
    "    # just taking the gradient in those cases\n",
    "    grads = [None for _ in grad_input]\n",
    "    grads[0] = torch.where(torch.abs(delta_in.repeat(dup0)) < 1e-6, grad_input[0],\n",
    "                           grad_output[0] * (delta_out / delta_in).repeat(dup0))\n",
    "    return tuple(grads)\n",
    "\n",
    "\n",
    "op_handler = {}\n",
    "\n",
    "# passthrough ops, where we make no change to the gradient\n",
    "op_handler['Dropout3d'] = passthrough\n",
    "op_handler['Dropout2d'] = passthrough\n",
    "op_handler['Dropout'] = passthrough\n",
    "op_handler['AlphaDropout'] = passthrough\n",
    "\n",
    "op_handler['Conv1d'] = linear_1d\n",
    "op_handler['Conv2d'] = linear_1d\n",
    "op_handler['Conv3d'] = linear_1d\n",
    "op_handler['ConvTranspose1d'] = linear_1d\n",
    "op_handler['ConvTranspose2d'] = linear_1d\n",
    "op_handler['ConvTranspose3d'] = linear_1d\n",
    "op_handler['Linear'] = linear_1d\n",
    "op_handler['AvgPool1d'] = linear_1d\n",
    "op_handler['AvgPool2d'] = linear_1d\n",
    "op_handler['AvgPool3d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool1d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool2d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool3d'] = linear_1d\n",
    "op_handler['BatchNorm1d'] = linear_1d\n",
    "op_handler['BatchNorm2d'] = linear_1d\n",
    "op_handler['BatchNorm3d'] = linear_1d\n",
    "\n",
    "op_handler['LeakyReLU'] = nonlinear_1d\n",
    "op_handler['ReLU'] = nonlinear_1d\n",
    "op_handler['ELU'] = nonlinear_1d\n",
    "op_handler['Sigmoid'] = nonlinear_1d\n",
    "op_handler[\"Tanh\"] = nonlinear_1d\n",
    "op_handler[\"Softplus\"] = nonlinear_1d\n",
    "op_handler['Softmax'] = nonlinear_1d\n",
    "\n",
    "op_handler['MaxPool1d'] = maxpool\n",
    "op_handler['MaxPool2d'] = maxpool\n",
    "op_handler['MaxPool3d'] = maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e6e82a8-0952-4250-8ecc-e29a5bc30f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The sampling procedure is modified for interpreting KL_PMVAE\n",
    "\"\"\"\n",
    "\n",
    "def splitExprandSample(condition_data, sampleSize, expr):\n",
    "    match_id = get_match_id(expr, condition_data)\n",
    "    split_expr = expr.iloc[match_id]\n",
    "    split_expr.index = range(0, split_expr.shape[0], 1)\n",
    "    split_expr = split_expr.T\n",
    "    split_expr = split_expr.sample(n=sampleSize, axis=1).T\n",
    "    return split_expr\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Specify if you want to obtain importance scores for genes/miRNA (i.e., input factors) (imp_feature = True, imp_mask = False) \n",
    "or pathways (imp_feature = False, imp_mask = True)\n",
    "\"\"\"\n",
    "\n",
    "def getTopShapValues(shap_vals, numberOfTopFeatures, path, file_count, featureNames, maskNames = None, imp_feature = True, imp_mask = False, absolute=True):\n",
    "    multiple_input = False\n",
    "    if type(shap_vals) == list:\n",
    "        multiple_input = True\n",
    "        shap_values = None\n",
    "        for l in range(len(shap_vals)):\n",
    "            if shap_values is not None:\n",
    "                shap_values = np.concatenate((shap_values, shap_vals[l]), axis=1)\n",
    "            else:\n",
    "                shap_values = shap_vals[l]\n",
    "        shap_vals = shap_values\n",
    "    \n",
    "    if absolute:\n",
    "        vals = np.abs(shap_vals).mean(0)\n",
    "    else:\n",
    "        vals = shap_vals.mean(0)\n",
    "    \n",
    "    if imp_feature:\n",
    "        feature_names = featureNames\n",
    "    if imp_mask:\n",
    "        feature_names = maskNames\n",
    "\n",
    "    feature_importance = pd.DataFrame(list(zip(feature_names, vals)),\n",
    "                                      columns=['features', 'importance_vals'])\n",
    "    feature_importance.sort_values(by=['importance_vals'], ascending=False, inplace=True)\n",
    "\n",
    "    mostImp_shap_values = feature_importance.head(numberOfTopFeatures)\n",
    "    print(mostImp_shap_values)\n",
    "\n",
    "    feature_importance.to_csv(path + \"/unsup_feature_imp_\" + file_count + \"_input_all_top6.csv\")\n",
    "    #feature_importance.to_csv(path + \"/unsup_feature_imp_\" + file_count + \"_1st_hidden_all_top6.csv\")\n",
    "    #feature_importance.to_csv(path + \"/unsup_feature_imp_specific_pathway_\" + file_count + \"_top6.csv\")\n",
    "    \"\"\"\n",
    "    print(mostImp_shap_values)\n",
    "    print(\"least importance absolute values\")\n",
    "    feature_importance.sort_values(by=['feature_importance_vals'], ascending=True, inplace=True)\n",
    "    leastImp_shap_values = feature_importance.head(numberOfTopFeatures)\n",
    "    print(leastImp_shap_values)\n",
    "    \"\"\"\n",
    "    return mostImp_shap_values\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "def UnSupShapExplainer(train_overall_df_gene, train_overall_df_mirna, condition_data1, condition_data2, path, file_count, dimension, featureNames, maskNames, imp_feature, imp_mask):\n",
    "    KL_PMVAE_model = KL_PMVAE_2omics(z_dim, input_n1, input_n2, pathway_mask_int)\n",
    "    \n",
    "    #load trained model\n",
    "    KL_PMVAE_model.load_state_dict(torch.load('saved_models/unsup_checkpoint_overall.pt', map_location=torch.device('cpu')))\n",
    "    \n",
    "    upper_group_sample_gene = splitExprandSample(condition_data=condition_data1, sampleSize=100, expr=train_overall_df_gene)\n",
    "    upper_mirna_match_id = get_match_id(train_overall_df_mirna, upper_group_sample_gene)\n",
    "    upper_group_sample_mirna = train_overall_df_mirna.iloc[upper_mirna_match_id]\n",
    "    \n",
    "    lower_group_sample_gene = splitExprandSample(condition_data=condition_data2, sampleSize=100, expr=train_overall_df_gene)\n",
    "    lower_mirna_match_id = get_match_id(train_overall_df_mirna, lower_group_sample_gene)\n",
    "    lower_group_sample_mirna = train_overall_df_mirna.iloc[lower_mirna_match_id]\n",
    "    \n",
    "    if imp_feature:\n",
    "        e = PyTorchDeepExplainer(KL_PMVAE_model, lower_group_sample_gene, lower_group_sample_mirna, outputNumber=0, dim=dimension, explainLatentSpace=True)\n",
    "    if imp_mask:\n",
    "        e = PyTorchDeepExplainer((KL_PMVAE_model, KL_PMVAE_model.e_fc2_mean), lower_group_sample_gene, lower_group_sample_mirna, outputNumber=0, dim=dimension, explainLatentSpace=True)\n",
    "    print(\"calculating shap values\")\n",
    "    shap_values_obtained = e.shap_values(upper_group_sample_gene, upper_group_sample_mirna)\n",
    "    \n",
    "    print(\"calculated shap values\")\n",
    "    most_imp  = getTopShapValues(shap_vals=shap_values_obtained, numberOfTopFeatures=10, path=path, file_count=file_count, featureNames=featureNames, maskNames=maskNames, imp_feature=imp_feature, imp_mask=imp_mask, absolute=True)\n",
    "    \n",
    "    return most_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a91930d6-2fd9-4889-9933-9d70d94bc325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent dimension: Z_5.\n",
      "4\n",
      "Latent dimension: Z_2.\n",
      "1\n",
      "Latent dimension: Z_4.\n",
      "3\n",
      "Latent dimension: Z_7.\n",
      "6\n",
      "Latent dimension: Z_1.\n",
      "0\n",
      "Latent dimension: Z_3.\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "important_latent = [\"Z_5\", \"Z_2\", \"Z_4\", \"Z_7\", \"Z_1\", \"Z_3\"]\n",
    "#z_count = np.array(list(range(1, 17, 1))).astype('str')\n",
    "#important_latent = np.char.add('Z_', z_count).tolist()\n",
    "imp_features_cache = []\n",
    "for i in important_latent:\n",
    "    print(\"Latent dimension: \" + i + \".\")\n",
    "    temp_dim = int(i.split(\"_\")[1])-1\n",
    "    print(temp_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "235a60a0-a82b-4da3-974b-44074d8e1e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent dimension: Z_5.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/sghoshstat/ywu39393/DKL/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "370   ENSG00000113578         0.021143\n",
      "1327  ENSG00000254087         0.020344\n",
      "2534  ENSG00000187288         0.020332\n",
      "1954  ENSG00000136869         0.019937\n",
      "2205  ENSG00000160654         0.019340\n",
      "1060  ENSG00000171608         0.018277\n",
      "2135  ENSG00000154589         0.015626\n",
      "264   ENSG00000103126         0.015389\n",
      "3091      hsa.mir.592         0.014744\n",
      "2655  ENSG00000240065         0.014656\n",
      "Latent dimension: Z_2.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/sghoshstat/ywu39393/DKL/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "1653  ENSG00000108821         0.037645\n",
      "393   ENSG00000115414         0.036143\n",
      "264   ENSG00000103126         0.025743\n",
      "1869  ENSG00000130635         0.025164\n",
      "2287  ENSG00000164692         0.023173\n",
      "83    ENSG00000060718         0.021086\n",
      "2783     hsa.mir.190b         0.020869\n",
      "1462  ENSG00000077420         0.020200\n",
      "2135  ENSG00000154589         0.019535\n",
      "1705  ENSG00000113569         0.019424\n",
      "Latent dimension: Z_4.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/sghoshstat/ywu39393/DKL/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "2205  ENSG00000160654         0.029316\n",
      "2783     hsa.mir.190b         0.023660\n",
      "700   ENSG00000141968         0.023651\n",
      "1060  ENSG00000171608         0.023564\n",
      "906   ENSG00000163519         0.021569\n",
      "1462  ENSG00000077420         0.021094\n",
      "3068     hsa.mir.5586         0.017622\n",
      "2755      hsa.mir.142         0.017605\n",
      "2770      hsa.mir.155         0.016460\n",
      "2763      hsa.mir.150         0.016348\n",
      "Latent dimension: Z_7.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/sghoshstat/ywu39393/DKL/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "923   ENSG00000163882         0.028869\n",
      "1276  ENSG00000205220         0.027949\n",
      "2511  ENSG00000185507         0.023108\n",
      "453   ENSG00000121879         0.022770\n",
      "700   ENSG00000141968         0.022699\n",
      "393   ENSG00000115414         0.016138\n",
      "1060  ENSG00000171608         0.015754\n",
      "162   ENSG00000082701         0.015580\n",
      "1705  ENSG00000113569         0.015194\n",
      "1722  ENSG00000115020         0.014938\n",
      "Latent dimension: Z_1.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/sghoshstat/ywu39393/DKL/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "2534  ENSG00000187288         0.051468\n",
      "2754      hsa.mir.139         0.030080\n",
      "3070     hsa.mir.5683         0.023582\n",
      "2832       hsa.mir.31         0.022527\n",
      "2796      hsa.mir.205         0.022061\n",
      "2957     hsa.mir.4501         0.019908\n",
      "1098  ENSG00000175445         0.019363\n",
      "2899     hsa.mir.3677         0.019044\n",
      "3099      hsa.mir.618         0.017046\n",
      "987   ENSG00000166851         0.016649\n",
      "Latent dimension: Z_3.\n",
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/sghoshstat/ywu39393/DKL/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "2655  ENSG00000240065         0.025090\n",
      "2783     hsa.mir.190b         0.020750\n",
      "987   ENSG00000166851         0.018287\n",
      "502   ENSG00000126067         0.014856\n",
      "1921  ENSG00000134480         0.013697\n",
      "370   ENSG00000113578         0.012963\n",
      "2711     hsa.mir.106b         0.012474\n",
      "3045      hsa.mir.511         0.012295\n",
      "764   ENSG00000147669         0.011870\n",
      "1061  ENSG00000171791         0.011847\n"
     ]
    }
   ],
   "source": [
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "identify key input factors (KIFs) (i.e., genes/miRNAs) for the important latent features;\n",
    "important latent features were found by applying DeepSHAP to trained LFSurv\n",
    "\"\"\"\n",
    "# dim is the location of it \n",
    "\n",
    "important_latent = [\"Z_5\", \"Z_2\", \"Z_4\", \"Z_7\", \"Z_1\", \"Z_3\"]\n",
    "#z_count = np.array(list(range(1, 17, 1))).astype('str')\n",
    "#important_latent = np.char.add('Z_', z_count).tolist()\n",
    "imp_features_cache = []\n",
    "for i in important_latent:\n",
    "    print(\"Latent dimension: \" + i + \".\")\n",
    "    temp_dim = int(i.split(\"_\")[1])-1\n",
    "    temp_most_imp_fea = UnSupShapExplainer(train_overall_df_gene=train_data_gene, train_overall_df_mirna=train_data_mirna, condition_data1=upper_data, condition_data2=lower_data, \n",
    "                                           path='saved_models', file_count = i,\n",
    "                                           dimension=temp_dim, featureNames=feature_names, maskNames=mask_names, imp_feature=True, imp_mask=False)\n",
    "    imp_features_cache = imp_features_cache + list(temp_most_imp_fea.features)\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "freq = {} \n",
    "for item in imp_features_cache: \n",
    "    if (item in freq): \n",
    "        freq[item] += 1\n",
    "    else: \n",
    "        freq[item] = 1\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "freq_summary = pd.DataFrame.from_dict(freq, orient ='index')\n",
    "freq_summary.columns = ['Frequency']\n",
    "freq_summary.sort_values(by=['Frequency'], ascending=False, inplace=True)\n",
    "big_freq_summary = freq_summary[freq_summary['Frequency'] > 1]\n",
    "\n",
    "big_freq_summary.to_csv(\"saved_models/top6Z_over1_impinput.csv\")\n",
    "#big_freq_summary.to_csv(\"saved_models/top6Z_over1_imp1sthidden.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e949f874-06d3-4735-b365-a2dab4119d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DeepSHAP\n",
    "modified version\n",
    "\"\"\"\n",
    "\n",
    "class Explainer(object):\n",
    "    \"\"\" This is the superclass of all explainers.\n",
    "    \"\"\"\n",
    "\n",
    "    def shap_values(self, X):\n",
    "        raise Exception(\"SHAP values not implemented for this explainer!\")\n",
    "\n",
    "    def attributions(self, X):\n",
    "        return self.shap_values(X)\n",
    "\n",
    "class PyTorchDeepExplainer(Explainer):\n",
    "    \"\"\"\n",
    "    This class has been adapted to explain OmiVAE. It is important that the correct output of the model\n",
    "    (0=z dimension, 2=mean) as well as the dimension within this latent space. We allow a dimension to be chosen to explain from, and adjustable outputs to be explained. \n",
    "    Lundberg et al., 2017: http://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, data1, data2, outputNumber, dim, explainLatentSpace):\n",
    "        \n",
    "        #data = list(load_data_deepshap(data, dtype))\n",
    "        data = list(load_data_deepshap(data1, data2, dtype))\n",
    "        \n",
    "        # check if we have multiple inputs\n",
    "        self.multi_input = False\n",
    "        if type(data) == list:\n",
    "            self.multi_input = True\n",
    "        else:\n",
    "            data = [data]\n",
    "        self.data = data\n",
    "        self.layer = None\n",
    "        self.input_handle = None\n",
    "        self.interim = False\n",
    "        self.interim_inputs_shape = None\n",
    "        self.expected_value = None  # to keep the DeepExplainer base happy\n",
    "        if type(model) == tuple:\n",
    "\n",
    "            self.interim = True\n",
    "            model, layer = model\n",
    "            model = model.eval()\n",
    "            self.layer = layer\n",
    "            self.add_target_handle(self.layer)\n",
    "\n",
    "            # if we are taking an interim layer, the 'data' is going to be the input\n",
    "            # of the interim layer; we will capture this using a forward hook\n",
    "            with torch.no_grad():\n",
    "                # _ = model(*data)\n",
    "                model.e_fc1.weight.data = model.e_fc1.weight.data.mul(model.pathway_mask)\n",
    "                _ = model.relu(model.e_bn1(model.e_fc1(data[0])))\n",
    "                interim_inputs = self.layer.target_input\n",
    "                print(\"Interim input type: %s.\" %type(interim_inputs))\n",
    "                if type(interim_inputs) is tuple:\n",
    "                    # this should always be true, but just to be safe\n",
    "                    self.interim_inputs_shape = [i.shape for i in interim_inputs]\n",
    "                else:\n",
    "                    self.interim_inputs_shape = [interim_inputs.shape]\n",
    "            self.target_handle.remove()\n",
    "            del self.layer.target_input\n",
    "        self.model = model.eval()\n",
    "        self.multi_output = False\n",
    "        self.num_outputs = 1\n",
    "        with torch.no_grad():\n",
    "            # outputs = model(*data)\n",
    "            model.e_fc1.weight.data = model.e_fc1.weight.data.mul(model.pathway_mask)\n",
    "            outputs = model.relu(model.e_bn1(model.e_fc1(data[0])))\n",
    "            #This is where specifies whether we want to explain the mean or z output\n",
    "            if type(outputs) != tuple:\n",
    "                output = outputs\n",
    "            else:\n",
    "                output = outputs[outputNumber]\n",
    "            self.outputNum=outputNumber\n",
    "            # Chosen dimension\n",
    "            self.dim=None\n",
    "            self.explainLatent = False\n",
    "            if explainLatentSpace:\n",
    "                self.explainLatent=True\n",
    "                self.dimension=dim\n",
    "                output = output[:, dim]\n",
    "                output = output.reshape(output.shape[0], 1)\n",
    "            # also get the device everything is running on\n",
    "            self.device = output.device\n",
    "            if output.shape[1] > 1:\n",
    "                self.multi_output = True\n",
    "                self.num_outputs = output.shape[1]\n",
    "            self.expected_value = output.mean(0).cpu().numpy()\n",
    "\n",
    "    def add_target_handle(self, layer):\n",
    "\n",
    "        input_handle = layer.register_forward_hook(get_target_input)\n",
    "        self.target_handle = input_handle\n",
    "\n",
    "    def add_handles(self, model, forward_handle, backward_handle):\n",
    "        \"\"\"\n",
    "        Add handles to all non-container layers in the model.\n",
    "        Recursively for non-container layers\n",
    "        \"\"\"\n",
    "        handles_list = []\n",
    "        model_children = list(model.children())\n",
    "        if model_children:\n",
    "            for child in model_children:\n",
    "                handles_list.extend(self.add_handles(child, forward_handle, backward_handle))\n",
    "        else:  # leaves\n",
    "            handles_list.append(model.register_forward_hook(forward_handle))\n",
    "            handles_list.append(model.register_backward_hook(backward_handle))\n",
    "\n",
    "        return handles_list\n",
    "\n",
    "    def remove_attributes(self, model):\n",
    "        \"\"\"\n",
    "        Removes the x and y attributes which were added by the forward handles\n",
    "        Recursively searches for non-container layers\n",
    "        \"\"\"\n",
    "        for child in model.children():\n",
    "            if 'nn.modules.container' in str(type(child)):\n",
    "                self.remove_attributes(child)\n",
    "            else:\n",
    "                try:\n",
    "                    del child.x\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "                try:\n",
    "                    del child.y\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "\n",
    "    def gradient(self, idx, inputs):\n",
    "\n",
    "        self.model.zero_grad()\n",
    "        X = [x.requires_grad_() for x in inputs]\n",
    "        \n",
    "        # output = self.model(*X)\n",
    "        self.model.e_fc1.weight.data = self.model.e_fc1.weight.data.mul(self.model.pathway_mask)\n",
    "        output = self.model.relu(self.model.e_bn1(self.model.e_fc1(X[0])))\n",
    "\n",
    "        #Specify the output to change\n",
    "        if type(output) != tuple:\n",
    "            outputs = output\n",
    "        else:\n",
    "            outputs = output[self.outputNum]\n",
    "\n",
    "        #Specify the dimension to explain\n",
    "        if self.explainLatent==True:\n",
    "\n",
    "            outputs = outputs[:, self.dimension]\n",
    "            outputs = outputs.reshape(outputs.shape[0], 1)\n",
    "\n",
    "\n",
    "        selected = [val for val in outputs[:, idx]]\n",
    "\n",
    "        grads = []\n",
    "        if self.interim:\n",
    "            interim_inputs = self.layer.target_input\n",
    "            for idx, input in enumerate(interim_inputs):\n",
    "                grad = torch.autograd.grad(selected, input,\n",
    "                                           retain_graph=True if idx + 1 < len(interim_inputs) else None,\n",
    "                                           allow_unused=True)[0]\n",
    "                if grad is not None:\n",
    "                    grad = grad.cpu().numpy()\n",
    "                else:\n",
    "                    grad = torch.zeros_like(interim_inputs[idx]).cpu().numpy()\n",
    "                grads.append(grad)\n",
    "            del self.layer.target_input\n",
    "            return grads, [i.detach().cpu().numpy() for i in interim_inputs]\n",
    "        else:\n",
    "            for idx, x in enumerate(X):\n",
    "                grad = torch.autograd.grad(selected, x,\n",
    "                                           retain_graph=True if idx + 1 < len(X) else None,\n",
    "                                           allow_unused=True)[0]\n",
    "                if grad is not None:\n",
    "                    grad = grad.cpu().numpy()\n",
    "                else:\n",
    "                    grad = torch.zeros_like(X[idx]).cpu().numpy()\n",
    "                grads.append(grad)\n",
    "            return grads\n",
    "\n",
    "    def shap_values(self, X1, X2, ranked_outputs=None, output_rank_order=\"max\", check_additivity=False):\n",
    "\n",
    "        # X ~ self.model_input\n",
    "        # X_data ~ self.data\n",
    "        \n",
    "        #X = list(load_data_deepshap(X, dtype))\n",
    "        X = list(load_data_deepshap(X1, X2, dtype))\n",
    "\n",
    "        # check if we have multiple inputs\n",
    "        if not self.multi_input:\n",
    "            assert type(X) != list, \"Expected a single tensor model input!\"\n",
    "            X = [X]\n",
    "        else:\n",
    "            assert type(X) == list, \"Expected a list of model inputs!\"\n",
    "\n",
    "\n",
    "        X = [x.detach().to(self.device) for x in X]\n",
    "\n",
    "        # if ranked output is given then this code is run and only the 'max' value given is explained\n",
    "        if ranked_outputs is not None and self.multi_output:\n",
    "            with torch.no_grad():\n",
    "                # model_output_values = self.model(*X)\n",
    "                self.model.e_fc1.weight.data = self.model.e_fc1.weight.data.mul(self.model.pathway_mask)\n",
    "                model_output_values = self.model.relu(self.model.e_bn1(self.model.e_fc1(X[0])))\n",
    "                \n",
    "                #Whithnell's change to adjust for the additional outputs in VAE model\n",
    "                model_output_values = model_output_values[self.outputNum]\n",
    "\n",
    "            # rank and determine the model outputs that we will explain\n",
    "\n",
    "            if output_rank_order == \"max\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=True)\n",
    "            elif output_rank_order == \"min\":\n",
    "                _, model_output_ranks = torch.sort(model_output_values, descending=False)\n",
    "            elif output_rank_order == \"max_abs\":\n",
    "                _, model_output_ranks = torch.sort(torch.abs(model_output_values), descending=True)\n",
    "            else:\n",
    "                assert False, \"output_rank_order must be max, min, or max_abs!\"\n",
    "\n",
    "        else:\n",
    "            # outputs an array of 0s so we know we are explaining the first value\n",
    "            model_output_ranks = (torch.ones((X[0].shape[0], self.num_outputs)).int() *\n",
    "                                  torch.arange(0, self.num_outputs).int())\n",
    "\n",
    "        # add the gradient handles\n",
    "\n",
    "        handles = self.add_handles(self.model, add_interim_values, deeplift_grad)\n",
    "        if self.interim:\n",
    "            self.add_target_handle(self.layer)\n",
    "\n",
    "        # compute the attributions\n",
    "        output_phis = []\n",
    "\n",
    "        for i in range(model_output_ranks.shape[1]):\n",
    "\n",
    "            phis = []\n",
    "            #phis are shapLundberg values\n",
    "\n",
    "            if self.interim:\n",
    "                for k in range(len(self.interim_inputs_shape)):\n",
    "                    phis.append(np.zeros((X[0].shape[0], ) + self.interim_inputs_shape[k][1: ]))\n",
    "            else:\n",
    "                for k in range(len(X)):\n",
    "                    phis.append(np.zeros(X[k].shape))\n",
    "            #shape is 5 as testing 5 samples\n",
    "            for j in range(X[0].shape[0]):\n",
    "\n",
    "                # tile the inputs to line up with the background data samples\n",
    "                tiled_X = [X[l][j:j + 1].repeat(\n",
    "                                   (self.data[l].shape[0],) + tuple([1 for k in range(len(X[l].shape) - 1)])) for l\n",
    "                           in range(len(X))]\n",
    "                joint_x = [torch.cat((tiled_X[l], self.data[l]), dim=0) for l in range(len(X))]\n",
    "                # run attribution computation graph\n",
    "                feature_ind = model_output_ranks[j, i]\n",
    "                sample_phis = self.gradient(feature_ind, joint_x)\n",
    "                # assign the attributions to the right part of the output arrays\n",
    "                if self.interim:\n",
    "                    sample_phis, output = sample_phis\n",
    "                    x, data = [], []\n",
    "                    for i in range(len(output)):\n",
    "                        x_temp, data_temp = np.split(output[i], 2)\n",
    "                        x.append(x_temp)\n",
    "                        data.append(data_temp)\n",
    "                    for l in range(len(self.interim_inputs_shape)):\n",
    "                        phis[l][j] = (sample_phis[l][self.data[l].shape[0]:] * (x[l] - data[l])).mean(0)\n",
    "                else:\n",
    "                    for l in range(len(X)):\n",
    "                        phis[l][j] = (torch.from_numpy(sample_phis[l][self.data[l].shape[0]:]).to(self.device) * (X[l][j: j + 1] - self.data[l])).cpu().numpy().mean(0)\n",
    "            output_phis.append(phis[0] if not self.multi_input else phis)\n",
    "\n",
    "\n",
    "        # cleanup; remove all gradient handles\n",
    "        for handle in handles:\n",
    "            handle.remove()\n",
    "        self.remove_attributes(self.model)\n",
    "        if self.interim:\n",
    "            self.target_handle.remove()\n",
    "\n",
    "        if not self.multi_output:\n",
    "            return output_phis[0]\n",
    "        elif ranked_outputs is not None:\n",
    "            # EW: returns a list... only want first value\n",
    "            return output_phis, model_output_ranks\n",
    "        else:\n",
    "            return output_phis\n",
    "\n",
    "# Module hooks\n",
    "\n",
    "\n",
    "def deeplift_grad(module, grad_input, grad_output):\n",
    "    \"\"\"The backward hook which computes the deeplift\n",
    "    gradient for an nn.Module\n",
    "    \"\"\"\n",
    "    # first, get the module type\n",
    "    module_type = module.__class__.__name__\n",
    "\n",
    "    # first, check the module is supported\n",
    "    if module_type in op_handler:\n",
    "\n",
    "        if op_handler[module_type].__name__ not in ['passthrough', 'linear_1d']:\n",
    "            return op_handler[module_type](module, grad_input, grad_output)\n",
    "    else:\n",
    "        print('Warning: unrecognized nn.Module: {}'.format(module_type))\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "def add_interim_values(module, input, output):\n",
    "    \"\"\"The forward hook used to save interim tensors, detached\n",
    "    from the graph. Used to calculate the multipliers\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del module.x\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    try:\n",
    "        del module.y\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    module_type = module.__class__.__name__\n",
    "\n",
    "    if module_type in op_handler:\n",
    "        func_name = op_handler[module_type].__name__\n",
    "\n",
    "        # First, check for cases where we don't need to save the x and y tensors\n",
    "        if func_name == 'passthrough':\n",
    "            pass\n",
    "        else:\n",
    "            # check only the 0th input varies\n",
    "            for i in range(len(input)):\n",
    "                if i != 0 and type(output) is tuple:\n",
    "                    assert input[i] == output[i], \"Only the 0th input may vary!\"\n",
    "            # if a new method is added, it must be added here too. This ensures tensors\n",
    "            # are only saved if necessary\n",
    "            if func_name in ['maxpool', 'nonlinear_1d']:\n",
    "                # only save tensors if necessary\n",
    "                if type(input) is tuple:\n",
    "                    setattr(module, 'x', torch.nn.Parameter(input[0].detach()))\n",
    "                else:\n",
    "                    setattr(module, 'x', torch.nn.Parameter(input.detach()))\n",
    "                if type(output) is tuple:\n",
    "                    setattr(module, 'y', torch.nn.Parameter(output[0].detach()))\n",
    "                else:\n",
    "                    setattr(module, 'y', torch.nn.Parameter(output.detach()))\n",
    "            if module_type in failure_case_modules:\n",
    "                input[0].register_hook(deeplift_tensor_grad)\n",
    "\n",
    "\n",
    "def get_target_input(module, input, output):\n",
    "    \"\"\"A forward hook which saves the tensor - attached to its graph.\n",
    "    Used if we want to explain the interim outputs of a model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        del module.target_input\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    setattr(module, 'target_input', input)\n",
    "\n",
    "# Whithnell:\n",
    "# From the documentation: \"The current implementation will not have the presented behavior for\n",
    "# complex Module that perform many operations. In some failure cases, grad_input and grad_output\n",
    "# will only contain the gradients for a subset of the inputs and outputs.\n",
    "# The tensor hook below handles such failure cases (currently, MaxPool1d). In such cases, the deeplift\n",
    "# grad should still be computed, and then appended to the complex_model_gradients list. The tensor hook\n",
    "# will then retrieve the proper gradient from this list.\n",
    "\n",
    "\n",
    "failure_case_modules = ['MaxPool1d']\n",
    "\n",
    "\n",
    "def deeplift_tensor_grad(grad):\n",
    "    return_grad = complex_module_gradients[-1]\n",
    "    del complex_module_gradients[-1]\n",
    "    return return_grad\n",
    "\n",
    "\n",
    "complex_module_gradients = []\n",
    "\n",
    "\n",
    "def passthrough(module, grad_input, grad_output):\n",
    "    \"\"\"No change made to gradients\"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def maxpool(module, grad_input, grad_output):\n",
    "    pool_to_unpool = {\n",
    "        'MaxPool1d': torch.nn.functional.max_unpool1d,\n",
    "        'MaxPool2d': torch.nn.functional.max_unpool2d,\n",
    "        'MaxPool3d': torch.nn.functional.max_unpool3d\n",
    "    }\n",
    "    pool_to_function = {\n",
    "        'MaxPool1d': torch.nn.functional.max_pool1d,\n",
    "        'MaxPool2d': torch.nn.functional.max_pool2d,\n",
    "        'MaxPool3d': torch.nn.functional.max_pool3d\n",
    "    }\n",
    "    delta_in = module.x[: int(module.x.shape[0] / 2)] - module.x[int(module.x.shape[0] / 2):]\n",
    "    dup0 = [2] + [1 for i in delta_in.shape[1:]]\n",
    "    # we also need to check if the output is a tuple\n",
    "    y, ref_output = torch.chunk(module.y, 2)\n",
    "    cross_max = torch.max(y, ref_output)\n",
    "    diffs = torch.cat([cross_max - ref_output, y - cross_max], 0)\n",
    "\n",
    "    # all of this just to unpool the outputs\n",
    "    with torch.no_grad():\n",
    "        _, indices = pool_to_function[module.__class__.__name__](\n",
    "            module.x, module.kernel_size, module.stride, module.padding,\n",
    "            module.dilation, module.ceil_mode, True)\n",
    "        xmax_pos, rmax_pos = torch.chunk(pool_to_unpool[module.__class__.__name__](\n",
    "            grad_output[0] * diffs, indices, module.kernel_size, module.stride,\n",
    "            module.padding, list(module.x.shape)), 2)\n",
    "    org_input_shape = grad_input[0].shape  # for the maxpool 1d\n",
    "    grad_input = [None for _ in grad_input]\n",
    "    grad_input[0] = torch.where(torch.abs(delta_in) < 1e-7, torch.zeros_like(delta_in),\n",
    "                           (xmax_pos + rmax_pos) / delta_in).repeat(dup0)\n",
    "    if module.__class__.__name__ == 'MaxPool1d':\n",
    "        complex_module_gradients.append(grad_input[0])\n",
    "        # the grad input that is returned doesn't matter, since it will immediately be\n",
    "        # be overridden by the grad in the complex_module_gradient\n",
    "        grad_input[0] = torch.ones(org_input_shape)\n",
    "    return tuple(grad_input)\n",
    "\n",
    "\n",
    "def linear_1d(module, grad_input, grad_output):\n",
    "    \"\"\"No change made to gradients.\"\"\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def nonlinear_1d(module, grad_input, grad_output):\n",
    "    delta_out = module.y[: int(module.y.shape[0] / 2)] - module.y[int(module.y.shape[0] / 2):]\n",
    "\n",
    "    delta_in = module.x[: int(module.x.shape[0] / 2)] - module.x[int(module.x.shape[0] / 2):]\n",
    "    dup0 = [2] + [1 for i in delta_in.shape[1:]]\n",
    "    # handles numerical instabilities where delta_in is very small by\n",
    "    # just taking the gradient in those cases\n",
    "    grads = [None for _ in grad_input]\n",
    "    grads[0] = torch.where(torch.abs(delta_in.repeat(dup0)) < 1e-6, grad_input[0],\n",
    "                           grad_output[0] * (delta_out / delta_in).repeat(dup0))\n",
    "    return tuple(grads)\n",
    "\n",
    "\n",
    "op_handler = {}\n",
    "\n",
    "# passthrough ops, where we make no change to the gradient\n",
    "op_handler['Dropout3d'] = passthrough\n",
    "op_handler['Dropout2d'] = passthrough\n",
    "op_handler['Dropout'] = passthrough\n",
    "op_handler['AlphaDropout'] = passthrough\n",
    "\n",
    "op_handler['Conv1d'] = linear_1d\n",
    "op_handler['Conv2d'] = linear_1d\n",
    "op_handler['Conv3d'] = linear_1d\n",
    "op_handler['ConvTranspose1d'] = linear_1d\n",
    "op_handler['ConvTranspose2d'] = linear_1d\n",
    "op_handler['ConvTranspose3d'] = linear_1d\n",
    "op_handler['Linear'] = linear_1d\n",
    "op_handler['AvgPool1d'] = linear_1d\n",
    "op_handler['AvgPool2d'] = linear_1d\n",
    "op_handler['AvgPool3d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool1d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool2d'] = linear_1d\n",
    "op_handler['AdaptiveAvgPool3d'] = linear_1d\n",
    "op_handler['BatchNorm1d'] = linear_1d\n",
    "op_handler['BatchNorm2d'] = linear_1d\n",
    "op_handler['BatchNorm3d'] = linear_1d\n",
    "\n",
    "op_handler['LeakyReLU'] = nonlinear_1d\n",
    "op_handler['ReLU'] = nonlinear_1d\n",
    "op_handler['ELU'] = nonlinear_1d\n",
    "op_handler['Sigmoid'] = nonlinear_1d\n",
    "op_handler[\"Tanh\"] = nonlinear_1d\n",
    "op_handler[\"Softplus\"] = nonlinear_1d\n",
    "op_handler['Softmax'] = nonlinear_1d\n",
    "\n",
    "op_handler['MaxPool1d'] = maxpool\n",
    "op_handler['MaxPool2d'] = maxpool\n",
    "op_handler['MaxPool3d'] = maxpool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab56083e-079f-48db-9043-55d53f79285c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating shap values\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/common/sghoshstat/ywu39393/DKL/lib/python3.9/site-packages/torch/nn/modules/module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculated shap values\n",
      "             features  importance_vals\n",
      "2431  ENSG00000177189         0.075969\n",
      "1688  ENSG00000112062         0.053806\n",
      "2508  ENSG00000185386         0.038046\n",
      "112   ENSG00000071242         0.005639\n",
      "369   ENSG00000113575         0.004694\n",
      "2147  ENSG00000155849         0.000000\n",
      "2138  ENSG00000154767         0.000000\n",
      "2139  ENSG00000155008         0.000000\n",
      "2140  ENSG00000155011         0.000000\n",
      "2141  ENSG00000155096         0.000000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Identify important genes for important pathways\n",
    "dim = location of this file_count in the layer \n",
    "\"\"\"\n",
    "\n",
    "# Remember to change model.output in PyTorchDeepExplainer() to the values of the first hidden layer before running the following command\n",
    "most_important_features = UnSupShapExplainer(train_overall_df_gene=train_data_gene, train_overall_df_mirna=train_data_mirna, condition_data1=upper_data, condition_data2=lower_data, \n",
    "                                             path='saved_models', file_count = \"R.HSA.163560\",\n",
    "                                             dimension=276, featureNames=feature_names, maskNames=mask_names, imp_feature=True, imp_mask=False)\n",
    "\n",
    "\n",
    "# In[ ]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44bca826-cfd8-4ea1-a451-d12f606e15c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      R-HSA-2142691  R-HSA-181429  R-HSA-381426  R-HSA-432142\n",
      "0                 0             0             0             1\n",
      "1                 0             0             0             0\n",
      "2                 0             0             0             0\n",
      "3                 0             0             0             0\n",
      "4                 0             0             0             0\n",
      "...             ...           ...           ...           ...\n",
      "2694              0             0             0             0\n",
      "2695              0             0             0             0\n",
      "2696              0             0             0             0\n",
      "2697              0             0             0             0\n",
      "2698              0             0             0             0\n",
      "\n",
      "[2699 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"processed_data_example/TCGA_BRCA/tune/minmax_normalized/pathway_mask.csv\")\n",
    "selected_columns = df.filter(like='142', axis=1)\n",
    "\n",
    "\n",
    "print(selected_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbf14992-a5a9-487f-99c6-b413bc623172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-HSA-2029482</th>\n",
       "      <th>R-HSA-5663213</th>\n",
       "      <th>R-HSA-201681</th>\n",
       "      <th>R-HSA-3238698</th>\n",
       "      <th>R-HSA-373080</th>\n",
       "      <th>R-HSA-4086400</th>\n",
       "      <th>R-HSA-4641262</th>\n",
       "      <th>R-HSA-163560</th>\n",
       "      <th>R-HSA-70895</th>\n",
       "      <th>R-HSA-193368</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2694</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2695</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2696</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2697</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2698</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2699 rows  10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      R-HSA-2029482  R-HSA-5663213  R-HSA-201681  R-HSA-3238698  R-HSA-373080  \\\n",
       "0                 0              0             0              0             0   \n",
       "1                 0              0             0              0             0   \n",
       "2                 0              0             0              0             0   \n",
       "3                 0              0             0              0             0   \n",
       "4                 0              0             0              0             0   \n",
       "...             ...            ...           ...            ...           ...   \n",
       "2694              0              0             0              0             0   \n",
       "2695              0              0             0              0             0   \n",
       "2696              0              0             0              0             0   \n",
       "2697              0              0             0              0             0   \n",
       "2698              0              0             0              0             0   \n",
       "\n",
       "      R-HSA-4086400  R-HSA-4641262  R-HSA-163560  R-HSA-70895  R-HSA-193368  \n",
       "0                 0              0             0            0             0  \n",
       "1                 0              0             0            0             0  \n",
       "2                 0              0             0            0             0  \n",
       "3                 0              0             0            0             0  \n",
       "4                 0              0             0            0             0  \n",
       "...             ...            ...           ...          ...           ...  \n",
       "2694              0              0             0            0             0  \n",
       "2695              0              0             0            0             0  \n",
       "2696              0              0             0            0             0  \n",
       "2697              0              0             0            0             0  \n",
       "2698              0              0             0            0             0  \n",
       "\n",
       "[2699 rows x 10 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[:, 270:280]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DKL",
   "language": "python",
   "name": "dkl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
